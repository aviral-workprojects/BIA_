{"file_contents":{"README.md":{"content":"# BIA - Bio-energy Intelligence Application\n\nA Streamlit-based bioenergy intelligence platform for waste-to-energy analysis with forecasting, financial modeling, and facility mapping.\n\n## Features\n\n- **User Authentication**: Secure signup/login with bcrypt password hashing\n- **Waste Logging**: Track waste generation with automatic date recording\n- **Forecasting**: Deterministic and SARIMA models for waste prediction\n- **Financial Analysis**: NPV, payback period, ROI calculations\n- **Sensitivity Analysis**: Tornado charts for parameter impact assessment\n- **Facility Mapping**: Interactive maps of bioenergy facilities\n- **Audit Trail**: Complete mathematical formulas and parameter provenance\n\n## Supported Cities\n\n- Ahmedabad\n- Gandhinagar  \n- Indore\n- Delhi\n- Mumbai\n- Pune\n- Bengaluru\n- Chennai\n\n## Quick Start\n\n1. **Install Dependencies**\n   ```bash\n   pip install streamlit pandas numpy plotly statsmodels pydantic scikit-learn folium streamlit-folium pydeck python-dateutil bcrypt pyyaml matplotlib\n   ```\n\n2. **Run Application**\n   ```bash\n   streamlit run app.py\n   ```\n\n3. **Access Application**\n   - Open your browser to `http://localhost:5000`\n   - Demo login: **demo** / **demo123**\n\n## Application Structure\n\n","size_bytes":1228},"app.py":{"content":"import streamlit as st\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport folium\nfrom streamlit_folium import st_folium\nimport json\nfrom datetime import datetime, date\nimport warnings\nimport os\n\n# Configuration flag for database backend\nUSE_SUPABASE = os.environ.get('DATABASE_URL') is not None\n\n# Import our modules - conditional based on database backend\nif USE_SUPABASE:\n    try:\n        from supabase_store import add_user, validate_user, add_waste_log, get_user_logs, migrate\n        # Run migration on startup\n        if not migrate():\n            st.sidebar.warning(\"Database migration failed. Using in-memory storage.\")\n            USE_SUPABASE = False\n    except Exception as e:\n        st.sidebar.warning(f\"Database connection failed. Using in-memory storage.\")\n        USE_SUPABASE = False\n\nif not USE_SUPABASE:\n    from auth_inmemory import AuthStore, add_user, validate_user, add_waste_log, get_user_logs\nelse:\n    # Create a dummy AuthStore class for Supabase mode\n    class AuthStore:\n        def __init__(self):\n            pass\nfrom bia_core.data_io import load_curated_data\nfrom bia_core.schemas import UserProfile, WasteLog\nfrom bia_core.features import create_forecast_features\nfrom bia_core.models import DeterministicModel, SARIMAModel, ModelSelector\nfrom bia_core.finance import FinanceCalculator\nfrom bia_core.eval import calculate_mape, backtest_model\nfrom bia_core.maps import create_facilities_map\nfrom bia_core.utils import format_currency, format_number, validate_range\n\nwarnings.filterwarnings('ignore')\n\n# Language translations\nTRANSLATIONS = {\n    \"en\": {\n        \"app_title\": \"BIA - Bio-energy Intelligence Application\",\n        \"welcome_message\": \"Welcome to the Bio-energy Intelligence Platform\",\n        \"login\": \"Login\",\n        \"signup\": \"Sign Up\",\n        \"username\": \"Username\",\n        \"password\": \"Password\",\n        \"entity_name\": \"Entity Name\",\n        \"city\": \"City\",\n        \"waste_type\": \"Waste Type\",\n        \"login_success\": \"Login successful!\",\n        \"invalid_credentials\": \"Invalid username or password\",\n        \"enter_credentials\": \"Please enter both username and password\",\n        \"demo_user\": \"Demo user: **demo** / **demo123**\",\n        \"account_created\": \"Account created successfully! Please login.\",\n        \"username_exists\": \"Username already exists\",\n        \"fill_fields\": \"Please fill in all fields\",\n        \"bia_controls\": \"BIA Controls\",\n        \"user\": \"User\",\n        \"entity\": \"Entity\",\n        \"technical_params\": \"Technical Parameters\",\n        \"yield_rate\": \"Yield Rate (kWh/ton)\",\n        \"capacity_factor\": \"Capacity Factor (%)\",\n        \"financial_params\": \"Financial Parameters\",\n        \"tariff\": \"Tariff (₹/kWh)\",\n        \"opex_per_ton\": \"OPEX per ton (₹)\",\n        \"fixed_opex\": \"Fixed OPEX (₹ lakhs/year)\",\n        \"capex\": \"CAPEX (₹ crores)\",\n        \"discount_rate\": \"Discount Rate (%)\",\n        \"project_horizon\": \"Project Horizon (years)\",\n        \"advanced_options\": \"Advanced Options\",\n        \"logout\": \"Logout\",\n        \"language\": \"Language\",\n        \"theme\": \"Theme\",\n        \"light\": \"Light\",\n        \"dark\": \"Dark\",\n        \"log_waste\": \"➕ Log Waste\",\n        \"waste_amount\": \"Waste Amount (tons)\",\n        \"date\": \"Date\",\n        \"add_waste_log\": \"Add Waste Log\",\n        \"waste_processed\": \"Waste Processed\",\n        \"energy_generated\": \"Energy Generated\",\n        \"co2_saved\": \"CO₂ Saved\",\n        \"dashboard\": \"Dashboard\",\n        \"entity_profile\": \"Entity Profile\",\n        \"waste_trend_forecast\": \"Waste Trend & Forecast\",\n        \"energy_finance\": \"Energy & Finance\",\n        \"npv_sensitivity\": \"NPV & Sensitivity\",\n        \"facilities_map\": \"Facilities Map\",\n        \"audit\": \"Audit\",\n        \"tons\": \"tons\",\n        \"kwh\": \"kWh\",\n        \"years\": \"years\",\n        \"optional_revenue\": \"Optional Revenue Streams\",\n        \"carbon_credits\": \"Carbon Credits (₹/credit)\",\n        \"carbon_credits_help\": \"Revenue from carbon credit sales (1 credit = 1 ton CO₂)\",\n        \"byproduct_sales\": \"Enable Byproduct Sales\",\n        \"byproduct_price\": \"Byproduct Price (₹/ton)\",\n        \"byproduct_help\": \"Revenue from digestate/compost sales\",\n        \"with_credits\": \"With Credits\",\n        \"without_credits\": \"Without Credits\",\n        \"byproduct_revenue\": \"Byproduct Revenue\"\n    },\n    \"hi\": {\n        \"app_title\": \"BIA - जैव-ऊर्जा बुद्धिमत्ता अनुप्रयोग\",\n        \"welcome_message\": \"जैव-ऊर्जा बुद्धिमत्ता मंच में आपका स्वागत है\",\n        \"login\": \"लॉगिन\",\n        \"signup\": \"साइन अप\",\n        \"username\": \"उपयोगकर्ता नाम\",\n        \"password\": \"पासवर्ड\",\n        \"entity_name\": \"संस्था का नाम\",\n        \"city\": \"शहर\",\n        \"waste_type\": \"अपशिष्ट प्रकार\",\n        \"login_success\": \"लॉगिन सफल!\",\n        \"invalid_credentials\": \"अमान्य उपयोगकर्ता नाम या पासवर्ड\",\n        \"enter_credentials\": \"कृपया उपयोगकर्ता नाम और पासवर्ड दर्ज करें\",\n        \"demo_user\": \"डेमो उपयोगकर्ता: **demo** / **demo123**\",\n        \"account_created\": \"खाता सफलतापूर्वक बनाया गया! कृपया लॉगिन करें।\",\n        \"username_exists\": \"उपयोगकर्ता नाम पहले से मौजूद है\",\n        \"fill_fields\": \"कृपया सभी फ़ील्ड भरें\",\n        \"bia_controls\": \"BIA नियंत्रण\",\n        \"user\": \"उपयोगकर्ता\",\n        \"entity\": \"संस्था\",\n        \"technical_params\": \"तकनीकी मापदंड\",\n        \"yield_rate\": \"उत्पादन दर (kWh/टन)\",\n        \"capacity_factor\": \"क्षमता कारक (%)\",\n        \"financial_params\": \"वित्तीय मापदंड\",\n        \"tariff\": \"टैरिफ (₹/kWh)\",\n        \"opex_per_ton\": \"OPEX प्रति टन (₹)\",\n        \"fixed_opex\": \"निश्चित OPEX (₹ लाख/वर्ष)\",\n        \"capex\": \"CAPEX (₹ करोड़)\",\n        \"discount_rate\": \"छूट दर (%)\",\n        \"project_horizon\": \"परियोजना अवधि (वर्ष)\",\n        \"advanced_options\": \"उन्नत विकल्प\",\n        \"logout\": \"लॉगआउट\",\n        \"language\": \"भाषा\",\n        \"theme\": \"थीम\",\n        \"light\": \"हल्का\",\n        \"dark\": \"गहरा\",\n        \"log_waste\": \"➕ अपशिष्ट लॉग करें\",\n        \"waste_amount\": \"अपशिष्ट मात्रा (टन)\",\n        \"date\": \"दिनांक\",\n        \"add_waste_log\": \"अपशिष्ट लॉग जोड़ें\",\n        \"waste_processed\": \"प्रसंस्कृत अपशिष्ट\",\n        \"energy_generated\": \"उत्पन्न ऊर्जा\",\n        \"co2_saved\": \"CO₂ बचत\",\n        \"dashboard\": \"डैशबोर्ड\",\n        \"entity_profile\": \"संस्था प्रोफ़ाइल\",\n        \"waste_trend_forecast\": \"अपशिष्ट प्रवृत्ति और पूर्वानुमान\",\n        \"energy_finance\": \"ऊर्जा और वित्त\",\n        \"npv_sensitivity\": \"NPV और संवेदनशीलता\",\n        \"facilities_map\": \"सुविधाएं मानचित्र\",\n        \"audit\": \"ऑडिट\",\n        \"tons\": \"टन\",\n        \"kwh\": \"kWh\",\n        \"years\": \"वर्ष\",\n        \"optional_revenue\": \"वैकल्पिक राजस्व स्रोत\",\n        \"carbon_credits\": \"कार्बन क्रेडिट (₹/क्रेडिट)\",\n        \"carbon_credits_help\": \"कार्बन क्रेडिट बिक्री से राजस्व (1 क्रेडिट = 1 टन CO₂)\",\n        \"byproduct_sales\": \"उप-उत्पाद बिक्री सक्षम करें\",\n        \"byproduct_price\": \"उप-उत्पाद मूल्य (₹/टन)\",\n        \"byproduct_help\": \"डाइजेस्टेट/कंपोस्ट बिक्री से राजस्व\",\n        \"with_credits\": \"क्रेडिट के साथ\",\n        \"without_credits\": \"क्रेडिट के बिना\",\n        \"byproduct_revenue\": \"उप-उत्पाद राजस्व\"\n    }\n}\n\ndef t(key):\n    \"\"\"Translation function\"\"\"\n    language = st.session_state.get('language', 'en')\n    return TRANSLATIONS.get(language, {}).get(key, key)\n\n# Page configuration\nst.set_page_config(\n    page_title=\"BIA - Bio-energy Intelligence Application\",\n    page_icon=\"🔋\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\"\n)\n\n# Constants\nSUPPORTED_CITIES = [\"Ahmedabad\", \"Gandhinagar\", \"Indore\", \"Delhi\", \"Mumbai\", \"Pune\", \"Bengaluru\", \"Chennai\"]\nWASTE_TYPES = [\"organic\", \"industrial\", \"agricultural\"]\n\n# Initialize auth store\nauth_store = AuthStore()\n\ndef init_session_state():\n    \"\"\"Initialize session state variables\"\"\"\n    if 'logged_in' not in st.session_state:\n        st.session_state.logged_in = False\n    if 'username' not in st.session_state:\n        st.session_state.username = None\n    if 'user_profile' not in st.session_state:\n        st.session_state.user_profile = None\n    if 'language' not in st.session_state:\n        st.session_state.language = 'en'\n    if 'theme' not in st.session_state:\n        st.session_state.theme = 'light'\n\ndef login_signup_page():\n    \"\"\"Handle login and signup\"\"\"\n    st.title(f\"🔋 {t('app_title')}\")\n    st.markdown(f\"### {t('welcome_message')}\")\n    \n    tab1, tab2 = st.tabs([t(\"login\"), t(\"signup\")])\n    \n    with tab1:\n        st.subheader(t(\"login\"))\n        with st.form(\"login_form\"):\n            username = st.text_input(t(\"username\"))\n            password = st.text_input(t(\"password\"), type=\"password\")\n            login_btn = st.form_submit_button(t(\"login\"))\n            \n            if login_btn:\n                if username and password:\n                    user_profile = validate_user(username, password)\n                    if user_profile:\n                        st.session_state.logged_in = True\n                        st.session_state.username = username\n                        st.session_state.user_profile = user_profile\n                        st.success(t(\"login_success\"))\n                        st.rerun()\n                    else:\n                        st.error(t(\"invalid_credentials\"))\n                else:\n                    st.error(t(\"enter_credentials\"))\n        \n        st.info(t(\"demo_user\"))\n    \n    with tab2:\n        st.subheader(t(\"signup\"))\n        with st.form(\"signup_form\"):\n            new_username = st.text_input(t(\"username\"), key=\"signup_username\")\n            new_password = st.text_input(t(\"password\"), type=\"password\", key=\"signup_password\")\n            entity_name = st.text_input(t(\"entity_name\"))\n            city = st.selectbox(t(\"city\"), SUPPORTED_CITIES)\n            waste_type = st.selectbox(t(\"waste_type\"), WASTE_TYPES)\n            signup_btn = st.form_submit_button(t(\"signup\"))\n            \n            if signup_btn:\n                if all([new_username, new_password, entity_name, city, waste_type]):\n                    if add_user(new_username, new_password, entity_name, city, waste_type):\n                        st.success(t(\"account_created\"))\n                    else:\n                        st.error(t(\"username_exists\"))\n                else:\n                    st.error(t(\"fill_fields\"))\n\ndef sidebar_controls():\n    \"\"\"Create sidebar controls for parameters\"\"\"\n    st.sidebar.title(f\"🔋 {t('bia_controls')}\")\n    \n    # Language and theme selector\n    col1, col2 = st.sidebar.columns(2)\n    with col1:\n        language = st.selectbox(\n            t(\"language\"),\n            options=[\"en\", \"hi\"],\n            format_func=lambda x: \"English\" if x == \"en\" else \"हिन्दी\",\n            index=0 if st.session_state.language == \"en\" else 1,\n            key=\"language_selector\"\n        )\n        if language != st.session_state.language:\n            st.session_state.language = language\n            st.rerun()\n    \n    with col2:\n        theme = st.selectbox(\n            t(\"theme\"),\n            options=[\"light\", \"dark\"],\n            format_func=lambda x: t(\"light\") if x == \"light\" else t(\"dark\"),\n            index=0 if st.session_state.theme == \"light\" else 1,\n            key=\"theme_selector\"\n        )\n        if theme != st.session_state.theme:\n            st.session_state.theme = theme\n            st.rerun()\n    \n    # Apply theme styling\n    if st.session_state.theme == \"dark\":\n        st.markdown(\"\"\"\n        <style>\n        .stApp { background-color: #0E1117; }\n        .stSidebar { background-color: #262730; }\n        </style>\n        \"\"\", unsafe_allow_html=True)\n    \n    # User info\n    if st.session_state.user_profile:\n        st.sidebar.write(f\"**{t('user')}:** {st.session_state.username}\")\n        st.sidebar.write(f\"**{t('entity')}:** {st.session_state.user_profile.entity_name}\")\n        st.sidebar.write(f\"**{t('city')}:** {st.session_state.user_profile.city}\")\n        st.sidebar.write(f\"**{t('waste_type')}:** {st.session_state.user_profile.waste_type}\")\n        \n        # Database backend indicator\n        db_status = \"🗄️ Database\" if USE_SUPABASE else \"💾 In-Memory\"\n        st.sidebar.write(f\"**Storage:** {db_status}\")\n    \n    st.sidebar.divider()\n    \n    # Basic parameters\n    st.sidebar.subheader(t(\"technical_params\"))\n    \n    yield_rate = st.sidebar.slider(\n        t(\"yield_rate\"), \n        min_value=100.0, max_value=2000.0, value=800.0, step=50.0,\n        help=\"Energy yield per ton of waste\"\n    )\n    \n    capacity_factor = st.sidebar.slider(\n        t(\"capacity_factor\"), \n        min_value=30.0, max_value=95.0, value=85.0, step=5.0,\n        help=\"Plant capacity utilization factor\"\n    ) / 100\n    \n    st.sidebar.subheader(t(\"financial_params\"))\n    \n    tariff = st.sidebar.slider(\n        t(\"tariff\"), \n        min_value=2.0, max_value=8.0, value=4.5, step=0.1,\n        help=\"Electricity selling price\"\n    )\n    \n    opex_per_ton = st.sidebar.slider(\n        t(\"opex_per_ton\"), \n        min_value=200.0, max_value=1000.0, value=500.0, step=50.0,\n        help=\"Operating expenses per ton of waste\"\n    )\n    \n    # Optional Revenue Streams\n    st.sidebar.subheader(f\"{t('optional_revenue')} (Optional)\")\n    \n    carbon_credit_price = st.sidebar.slider(\n        t(\"carbon_credits\"),\n        min_value=0.0, max_value=1000.0, value=300.0, step=10.0,\n        help=t(\"carbon_credits_help\")\n    )\n    \n    enable_byproduct = st.sidebar.checkbox(\n        t(\"byproduct_sales\"),\n        value=False,\n        help=t(\"byproduct_help\")\n    )\n    \n    byproduct_price = 0.0\n    if enable_byproduct:\n        byproduct_price = st.sidebar.slider(\n            t(\"byproduct_price\"),\n            min_value=0.0, max_value=500.0, value=50.0, step=5.0,\n            help=t(\"byproduct_help\")\n        )\n    \n    # Advanced options in expander\n    with st.sidebar.expander(t(\"advanced_options\")):\n        fixed_opex = st.number_input(\n            t(\"fixed_opex\"), \n            min_value=0.0, max_value=100.0, value=10.0, step=1.0,\n            help=\"Fixed annual operating expenses\"\n        ) * 100000  # Convert to rupees\n        \n        capex = st.number_input(\n            t(\"capex\"), \n            min_value=1.0, max_value=100.0, value=25.0, step=1.0,\n            help=\"Capital expenditure\"\n        ) * 10000000  # Convert to rupees\n        \n        discount_rate = st.slider(\n            t(\"discount_rate\"), \n            min_value=5.0, max_value=20.0, value=12.0, step=0.5,\n            help=\"Cost of capital\"\n        ) / 100\n        \n        horizon_years = st.slider(\n            t(\"project_horizon\"), \n            min_value=10, max_value=30, value=20, step=1,\n            help=\"Project lifetime\"\n        )\n    \n    st.sidebar.divider()\n    \n    # Logout button\n    if st.sidebar.button(t(\"logout\"), type=\"secondary\"):\n        st.session_state.logged_in = False\n        st.session_state.username = None\n        st.session_state.user_profile = None\n        st.rerun()\n    \n    return {\n        'yield_rate': yield_rate,\n        'capacity_factor': capacity_factor,\n        'tariff': tariff,\n        'opex_per_ton': opex_per_ton,\n        'fixed_opex': fixed_opex,\n        'capex': capex,\n        'discount_rate': discount_rate,\n        'horizon_years': horizon_years,\n        'carbon_credit_price': carbon_credit_price,\n        'enable_byproduct': enable_byproduct,\n        'byproduct_price': byproduct_price\n    }\n\ndef waste_logging_section():\n    \"\"\"Waste logging interface\"\"\"\n    st.subheader(t(\"log_waste\"))\n    \n    with st.form(\"waste_log_form\"):\n        col1, col2 = st.columns(2)\n        with col1:\n            waste_amount = st.number_input(\n                t(\"waste_amount\"), \n                min_value=0.01, max_value=1000.0, value=1.0, step=0.1\n            )\n        with col2:\n            log_date = st.date_input(t(\"date\"), value=date.today())\n        \n        submit_btn = st.form_submit_button(t(\"add_waste_log\"))\n        \n        if submit_btn:\n            if waste_amount > 0:\n                waste_log = WasteLog(\n                    username=st.session_state.username,\n                    date=log_date,\n                    waste_tons=waste_amount\n                )\n                add_waste_log(waste_log)\n                st.success(f\"Added {waste_amount} {t('tons')} of waste for {log_date}\")\n                st.rerun()\n            else:\n                st.error(\"Please enter a valid waste amount\")\n\ndef entity_profile_tab():\n    \"\"\"Entity profile and logs\"\"\"\n    st.header(\"🏢 Entity Profile\")\n    \n    profile = st.session_state.user_profile\n    \n    # Profile information\n    col1, col2 = st.columns(2)\n    with col1:\n        st.info(f\"\"\"\n        **Entity Name:** {profile.entity_name}  \n        **City:** {profile.city}  \n        **Waste Type:** {profile.waste_type}  \n        **Username:** {st.session_state.username}\n        \"\"\")\n    \n    with col2:\n        waste_logging_section()\n    \n    # Waste logs\n    st.subheader(\"📊 Waste Logs\")\n    logs = get_user_logs(st.session_state.username)\n    \n    if logs:\n        df_logs = pd.DataFrame([{\n            'Date': log.date,\n            'Waste (tons)': log.waste_tons\n        } for log in logs])\n        \n        # Summary metrics\n        col1, col2, col3 = st.columns(3)\n        with col1:\n            st.metric(\"Total Waste\", f\"{df_logs['Waste (tons)'].sum():.2f} tons\")\n        with col2:\n            today_waste = df_logs[df_logs['Date'] == date.today()]['Waste (tons)'].sum()\n            st.metric(\"Today's Waste\", f\"{today_waste:.2f} tons\")\n        with col3:\n            st.metric(\"Total Logs\", len(df_logs))\n        \n        # Logs table\n        st.dataframe(df_logs, use_container_width=True)\n        \n        # Download logs\n        csv = df_logs.to_csv(index=False)\n        st.download_button(\n            label=\"Download Logs CSV\",\n            data=csv,\n            file_name=f\"waste_logs_{st.session_state.username}.csv\",\n            mime=\"text/csv\"\n        )\n    else:\n        st.info(\"No waste logs found. Add some logs to get started!\")\n\ndef forecast_tab(params):\n    \"\"\"Waste trend and forecast\"\"\"\n    st.header(\"📈 Waste Trend & Forecast\")\n    \n    logs = get_user_logs(st.session_state.username)\n    \n    if not logs or len(logs) < 2:\n        st.warning(\"Need at least 2 waste logs to generate forecasts\")\n        return\n    \n    # Prepare data\n    df_logs = pd.DataFrame([{\n        'date': log.date,\n        'waste_tons': log.waste_tons\n    } for log in logs])\n    \n    df_logs = df_logs.sort_values('date')\n    df_logs['cumulative_waste'] = df_logs['waste_tons'].cumsum()\n    \n    # Create forecast features\n    forecast_features = create_forecast_features(df_logs)\n    \n    # Initialize models\n    det_model = DeterministicModel()\n    sarima_model = SARIMAModel()\n    \n    # Fit models\n    det_model.fit(forecast_features)\n    sarima_model.fit(forecast_features)\n    \n    # Generate forecasts\n    forecast_days = 30\n    det_forecast = det_model.predict(forecast_days)\n    sarima_forecast = sarima_model.predict(forecast_days)\n    \n    # Model selection based on backtest\n    model_selector = ModelSelector([det_model, sarima_model])\n    best_model = model_selector.select_best_model(forecast_features)\n    \n    # Visualizations\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        # Historical trend\n        fig_hist = px.line(df_logs, x='date', y='waste_tons', \n                          title=\"Historical Waste Logs\")\n        fig_hist.update_layout(xaxis_title=\"Date\", yaxis_title=\"Waste (tons)\")\n        st.plotly_chart(fig_hist, use_container_width=True)\n    \n    with col2:\n        # Cumulative waste\n        fig_cum = px.line(df_logs, x='date', y='cumulative_waste', \n                         title=\"Cumulative Waste\")\n        fig_cum.update_layout(xaxis_title=\"Date\", yaxis_title=\"Cumulative Waste (tons)\")\n        st.plotly_chart(fig_cum, use_container_width=True)\n    \n    # Forecast comparison\n    st.subheader(\"📊 Forecast Comparison\")\n    \n    forecast_dates = pd.date_range(\n        start=df_logs['date'].max() + pd.Timedelta(days=1),\n        periods=forecast_days,\n        freq='D'\n    )\n    \n    forecast_df = pd.DataFrame({\n        'Date': forecast_dates,\n        'Deterministic': det_forecast,\n        'SARIMA': sarima_forecast\n    })\n    \n    fig_forecast = go.Figure()\n    \n    # Add historical data\n    fig_forecast.add_trace(go.Scatter(\n        x=df_logs['date'], y=df_logs['waste_tons'],\n        mode='lines+markers', name='Historical',\n        line=dict(color='blue')\n    ))\n    \n    # Add forecasts\n    fig_forecast.add_trace(go.Scatter(\n        x=forecast_df['Date'], y=forecast_df['Deterministic'],\n        mode='lines', name='Deterministic Forecast',\n        line=dict(color='red', dash='dash')\n    ))\n    \n    fig_forecast.add_trace(go.Scatter(\n        x=forecast_df['Date'], y=forecast_df['SARIMA'],\n        mode='lines', name='SARIMA Forecast',\n        line=dict(color='green', dash='dot')\n    ))\n    \n    fig_forecast.update_layout(\n        title=\"Waste Forecast (30 days)\",\n        xaxis_title=\"Date\",\n        yaxis_title=\"Waste (tons)\"\n    )\n    \n    st.plotly_chart(fig_forecast, use_container_width=True)\n    \n    # Model performance\n    st.subheader(\"🎯 Model Performance\")\n    \n    if len(forecast_features) >= 10:  # Need sufficient data for backtest\n        det_mape = backtest_model(det_model, forecast_features)\n        sarima_mape = backtest_model(sarima_model, forecast_features)\n        \n        perf_df = pd.DataFrame({\n            'Model': ['Deterministic', 'SARIMA'],\n            'MAPE (%)': [det_mape, sarima_mape],\n            'Selected': ['✓' if best_model == det_model else '✗',\n                        '✓' if best_model == sarima_model else '✗']\n        })\n        \n        st.dataframe(perf_df, use_container_width=True)\n        \n        st.info(f\"**Best Model:** {type(best_model).__name__} (Lower MAPE is better)\")\n    else:\n        st.info(\"Need more data points for model backtesting\")\n\ndef energy_finance_tab(params):\n    \"\"\"Energy and finance calculations\"\"\"\n    st.header(\"⚡ Energy & Finance\")\n    \n    logs = get_user_logs(st.session_state.username)\n    \n    if not logs:\n        st.warning(\"No waste logs found. Add some logs to see energy and finance projections.\")\n        return\n    \n    # Calculate totals\n    total_waste = sum(log.waste_tons for log in logs)\n    today_waste = sum(log.waste_tons for log in logs if log.date == date.today())\n    \n    # Energy calculations\n    gross_electricity = total_waste * params['yield_rate'] * params['capacity_factor']\n    estimated_daily_electricity = today_waste * params['yield_rate'] * params['capacity_factor']\n    \n    # Display metrics\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        st.metric(\n            \"Gross Electricity Generated\", \n            f\"{format_number(gross_electricity)} kWh\",\n            help=\"Total electricity from all logged waste\"\n        )\n    \n    with col2:\n        st.metric(\n            \"Today's Est. Electricity\", \n            f\"{format_number(estimated_daily_electricity)} kWh\",\n            help=\"Estimated electricity from today's waste\"\n        )\n    \n    with col3:\n        revenue_potential = gross_electricity * params['tariff']\n        st.metric(\n            \"Revenue Potential\", \n            f\"₹{format_number(revenue_potential)}\",\n            help=\"Revenue from gross electricity\"\n        )\n    \n    # Financial projections\n    st.subheader(\"💰 Financial Projections\")\n    \n    # Initialize finance calculator\n    calc = FinanceCalculator(\n        yield_rate=params['yield_rate'],\n        capacity_factor=params['capacity_factor'],\n        tariff=params['tariff'],\n        opex_per_ton=params['opex_per_ton'],\n        fixed_opex=params['fixed_opex'],\n        capex=params['capex'],\n        discount_rate=params['discount_rate'],\n        carbon_credit_price=params['carbon_credit_price'],\n        byproduct_price=params['byproduct_price'],\n        enable_byproduct=params['enable_byproduct']\n    )\n    \n    # Calculate average daily waste for projections\n    if logs:\n        avg_daily_waste = total_waste / len(set(log.date for log in logs))\n    else:\n        avg_daily_waste = 1.0  # Default\n    \n    # Generate cashflows\n    cashflows = calc.calculate_cashflows(avg_daily_waste, params['horizon_years'])\n    \n    # Create cashflow chart\n    years = list(range(1, params['horizon_years'] + 1))\n    \n    fig_cf = go.Figure()\n    \n    # Split revenue into components for stacked bar chart\n    electricity_revenues = [cf['electricity_revenue'] for cf in cashflows]\n    carbon_revenues = [cf['carbon_revenue'] for cf in cashflows] \n    byproduct_revenues = [cf['byproduct_revenue'] for cf in cashflows]\n    \n    fig_cf.add_trace(go.Bar(\n        x=years, y=electricity_revenues,\n        name='Electricity Revenue', marker_color='green'\n    ))\n    \n    fig_cf.add_trace(go.Bar(\n        x=years, y=carbon_revenues,\n        name='Carbon Credits', marker_color='lightgreen'\n    ))\n    \n    if params['enable_byproduct']:\n        fig_cf.add_trace(go.Bar(\n            x=years, y=byproduct_revenues,\n            name=t('byproduct_revenue'), marker_color='darkgreen'\n        ))\n    \n    fig_cf.add_trace(go.Bar(\n        x=years, y=[-cf['opex'] for cf in cashflows],\n        name='OPEX', marker_color='red'\n    ))\n    \n    fig_cf.add_trace(go.Scatter(\n        x=years, y=[cf['ncf'] for cf in cashflows],\n        mode='lines+markers', name='Net Cash Flow',\n        line=dict(color='blue', width=3)\n    ))\n    \n    fig_cf.update_layout(\n        title=\"Annual Cashflows\",\n        xaxis_title=\"Year\",\n        yaxis_title=\"Amount (₹)\",\n        barmode='stack'\n    )\n    \n    st.plotly_chart(fig_cf, use_container_width=True)\n    \n    # Export cashflows\n    cf_df = pd.DataFrame([{\n        'Year': i+1,\n        'Waste (tons)': cf['waste_tons'],\n        'Electricity (kWh)': cf['electricity_kwh'],\n        'Electricity Revenue (₹)': cf['electricity_revenue'],\n        'Carbon Credits Revenue (₹)': cf['carbon_revenue'],\n        'Byproduct Revenue (₹)': cf['byproduct_revenue'],\n        'Total Revenue (₹)': cf['revenue'],\n        'OPEX (₹)': cf['opex'],\n        'Net Cash Flow (₹)': cf['ncf']\n    } for i, cf in enumerate(cashflows)])\n    \n    st.subheader(\"📋 Annual Cashflow Table\")\n    st.dataframe(cf_df, use_container_width=True)\n    \n    # Download button\n    csv = cf_df.to_csv(index=False)\n    st.download_button(\n        label=\"Download Cashflows CSV\",\n        data=csv,\n        file_name=f\"cashflows_{st.session_state.username}.csv\",\n        mime=\"text/csv\"\n    )\n\ndef npv_sensitivity_tab(params):\n    \"\"\"NPV calculations and sensitivity analysis\"\"\"\n    st.header(\"💹 NPV & Sensitivity Analysis\")\n    \n    logs = get_user_logs(st.session_state.username)\n    \n    if not logs:\n        st.warning(\"No waste logs found. Add some logs to see NPV analysis.\")\n        return\n    \n    # Calculate average daily waste\n    total_waste = sum(log.waste_tons for log in logs)\n    avg_daily_waste = total_waste / len(set(log.date for log in logs))\n    \n    # Initialize finance calculator\n    calc = FinanceCalculator(\n        yield_rate=params['yield_rate'],\n        capacity_factor=params['capacity_factor'],\n        tariff=params['tariff'],\n        opex_per_ton=params['opex_per_ton'],\n        fixed_opex=params['fixed_opex'],\n        capex=params['capex'],\n        discount_rate=params['discount_rate'],\n        carbon_credit_price=params['carbon_credit_price'],\n        byproduct_price=params['byproduct_price'],\n        enable_byproduct=params['enable_byproduct']\n    )\n    \n    # Calculate base case metrics\n    npv = calc.calculate_npv(avg_daily_waste, params['horizon_years'])\n    payback = calc.calculate_payback(avg_daily_waste, params['horizon_years'])\n    roi = calc.calculate_roi(avg_daily_waste, params['horizon_years'])\n    \n    # CO2 savings calculation\n    total_kwh = sum(log.waste_tons for log in logs) * params['yield_rate'] * params['capacity_factor']\n    co2_savings = (total_kwh * 0.9) / 1000  # kg to tons\n    trees_equivalent = 50 * co2_savings\n    \n    # Display metrics\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"NPV\", f\"₹{format_currency(npv)}\")\n    \n    with col2:\n        st.metric(\"Payback Period\", f\"{payback:.1f} years\" if payback != float('inf') else \"∞\")\n    \n    with col3:\n        st.metric(\"ROI\", f\"{roi:.1f}%\")\n    \n    with col4:\n        st.metric(\"CO₂ Savings\", f\"{co2_savings:.1f} tons\")\n    \n    # Environmental impact\n    st.subheader(\"🌱 Environmental Impact\")\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        st.metric(\"CO₂ Savings\", f\"{co2_savings:.2f} tons\")\n    \n    with col2:\n        st.metric(\"Trees Equivalent\", f\"{trees_equivalent:.0f} trees\")\n    \n    # Sensitivity Analysis\n    st.subheader(\"📊 Sensitivity Analysis\")\n    \n    # Parameters to analyze\n    sensitivity_params = {\n        'Yield Rate': ('yield_rate', params['yield_rate']),\n        'Tariff': ('tariff', params['tariff']),\n        'OPEX per ton': ('opex_per_ton', params['opex_per_ton']),\n        'CAPEX': ('capex', params['capex']),\n        'Discount Rate': ('discount_rate', params['discount_rate'])\n    }\n    \n    # Calculate sensitivity\n    sensitivity_results = []\n    \n    for param_name, (param_key, base_value) in sensitivity_params.items():\n        # +15% case\n        params_high = params.copy()\n        params_high[param_key] = base_value * 1.15\n        \n        calc_high = FinanceCalculator(\n            yield_rate=params_high['yield_rate'],\n            capacity_factor=params_high['capacity_factor'],\n            tariff=params_high['tariff'],\n            opex_per_ton=params_high['opex_per_ton'],\n            fixed_opex=params_high['fixed_opex'],\n            capex=params_high['capex'],\n            discount_rate=params_high['discount_rate'],\n            carbon_credit_price=params_high.get('carbon_credit_price', params['carbon_credit_price']),\n            byproduct_price=params_high.get('byproduct_price', params['byproduct_price']),\n            enable_byproduct=params_high.get('enable_byproduct', params['enable_byproduct'])\n        )\n        \n        npv_high = calc_high.calculate_npv(avg_daily_waste, params['horizon_years'])\n        \n        # -15% case\n        params_low = params.copy()\n        params_low[param_key] = base_value * 0.85\n        \n        calc_low = FinanceCalculator(\n            yield_rate=params_low['yield_rate'],\n            capacity_factor=params_low['capacity_factor'],\n            tariff=params_low['tariff'],\n            opex_per_ton=params_low['opex_per_ton'],\n            fixed_opex=params_low['fixed_opex'],\n            capex=params_low['capex'],\n            discount_rate=params_low['discount_rate'],\n            carbon_credit_price=params_low.get('carbon_credit_price', params['carbon_credit_price']),\n            byproduct_price=params_low.get('byproduct_price', params['byproduct_price']),\n            enable_byproduct=params_low.get('enable_byproduct', params['enable_byproduct'])\n        )\n        \n        npv_low = calc_low.calculate_npv(avg_daily_waste, params['horizon_years'])\n        \n        # Calculate sensitivity\n        npv_range = npv_high - npv_low\n        \n        sensitivity_results.append({\n            'Parameter': param_name,\n            'NPV Impact': npv_range,\n            'NPV Low (-15%)': npv_low,\n            'NPV High (+15%)': npv_high\n        })\n    \n    # Sort by impact\n    sensitivity_results.sort(key=lambda x: abs(x['NPV Impact']), reverse=True)\n    \n    # Create tornado chart\n    fig_tornado = go.Figure()\n    \n    params_list = [r['Parameter'] for r in sensitivity_results]\n    impacts = [r['NPV Impact'] for r in sensitivity_results]\n    \n    fig_tornado.add_trace(go.Bar(\n        y=params_list,\n        x=impacts,\n        orientation='h',\n        marker_color=['red' if x < 0 else 'green' for x in impacts]\n    ))\n    \n    fig_tornado.update_layout(\n        title=\"NPV Sensitivity Analysis (±15% parameter change)\",\n        xaxis_title=\"NPV Impact (₹)\",\n        yaxis_title=\"Parameter\"\n    )\n    \n    st.plotly_chart(fig_tornado, use_container_width=True)\n    \n    # Sensitivity table\n    st.subheader(\"📋 Sensitivity Results\")\n    sens_df = pd.DataFrame(sensitivity_results)\n    st.dataframe(sens_df, use_container_width=True)\n    \n    # Optional Revenue Impact Comparison\n    if params['carbon_credit_price'] > 0 or params['enable_byproduct']:\n        st.subheader(\"🌱 Revenue Stream Comparison\")\n        \n        # Calculate base scenario (no carbon credits, no byproduct)\n        calc_base = FinanceCalculator(\n            yield_rate=params['yield_rate'],\n            capacity_factor=params['capacity_factor'],\n            tariff=params['tariff'],\n            opex_per_ton=params['opex_per_ton'],\n            fixed_opex=params['fixed_opex'],\n            capex=params['capex'],\n            discount_rate=params['discount_rate'],\n            carbon_credit_price=0.0,\n            byproduct_price=0.0,\n            enable_byproduct=False\n        )\n        \n        npv_base = calc_base.calculate_npv(avg_daily_waste, params['horizon_years'])\n        npv_with_extras = calc.calculate_npv(avg_daily_waste, params['horizon_years'])\n        npv_improvement = npv_with_extras - npv_base\n        \n        col1, col2, col3 = st.columns(3)\n        with col1:\n            st.metric(\"Base NPV (Electricity Only)\", f\"₹{format_currency(npv_base)}\")\n        with col2:\n            st.metric(\"NPV with Extras\", f\"₹{format_currency(npv_with_extras)}\")\n        with col3:\n            st.metric(\"Improvement\", f\"₹{format_currency(npv_improvement)}\", \n                     delta=f\"{((npv_improvement/npv_base)*100):.1f}%\" if npv_base != 0 else \"N/A\")\n        \n        # Show revenue breakdown\n        cashflows = calc.calculate_cashflows(avg_daily_waste, params['horizon_years'])\n        total_electricity_rev = sum(cf['electricity_revenue'] for cf in cashflows)\n        total_carbon_rev = sum(cf['carbon_revenue'] for cf in cashflows)\n        total_byproduct_rev = sum(cf['byproduct_revenue'] for cf in cashflows)\n        \n        st.write(\"**Revenue Breakdown over Project Life:**\")\n        revenue_breakdown = pd.DataFrame({\n            'Revenue Stream': ['Electricity Sales', 'Carbon Credits', 'Byproduct Sales'],\n            'Total (₹)': [total_electricity_rev, total_carbon_rev, total_byproduct_rev],\n            'Percentage': [\n                f\"{(total_electricity_rev/(total_electricity_rev + total_carbon_rev + total_byproduct_rev)*100):.1f}%\",\n                f\"{(total_carbon_rev/(total_electricity_rev + total_carbon_rev + total_byproduct_rev)*100):.1f}%\",\n                f\"{(total_byproduct_rev/(total_electricity_rev + total_carbon_rev + total_byproduct_rev)*100):.1f}%\"\n            ]\n        })\n        st.dataframe(revenue_breakdown, use_container_width=True)\n\ndef facilities_map_tab():\n    \"\"\"Facilities mapping\"\"\"\n    st.header(\"🗺️ Facilities Map\")\n    \n    # Load facilities data\n    try:\n        facilities_data = load_curated_data()['facilities']\n        user_city = st.session_state.user_profile.city\n        \n        # Filter facilities for user's city\n        city_facilities = facilities_data[facilities_data['city'] == user_city]\n        \n        if len(city_facilities) > 0:\n            # Create map\n            facilities_map = create_facilities_map(city_facilities, user_city)\n            \n            # Display map\n            map_data = st_folium(facilities_map, width=700, height=500)\n            \n            # Display facilities table\n            st.subheader(f\"🏭 Facilities in {user_city}\")\n            \n            display_df = city_facilities[['name', 'type', 'capacity_mw', 'status', 'source']].copy()\n            display_df.columns = ['Name', 'Type', 'Capacity (MW)', 'Status', 'Source']\n            \n            st.dataframe(display_df, use_container_width=True)\n            \n            # Summary statistics\n            col1, col2, col3 = st.columns(3)\n            \n            with col1:\n                st.metric(\"Total Facilities\", len(city_facilities))\n            \n            with col2:\n                total_capacity = city_facilities['capacity_mw'].sum()\n                st.metric(\"Total Capacity\", f\"{total_capacity:.1f} MW\")\n            \n            with col3:\n                operational_count = len(city_facilities[city_facilities['status'] == 'operational'])\n                st.metric(\"Operational\", operational_count)\n            \n        else:\n            st.info(f\"No verified facilities found in {user_city} in our current database.\")\n            st.markdown(\"**Note:** Our facility database is curated from verified sources and may not include all facilities.\")\n            \n    except Exception as e:\n        st.error(f\"Error loading facilities data: {str(e)}\")\n        st.info(\"Please check if the facilities data file is available and properly formatted.\")\n\ndef audit_tab(params):\n    \"\"\"Audit trail with formulas and parameters\"\"\"\n    st.header(\"🔍 Audit Trail\")\n    \n    # LaTeX formulas\n    st.subheader(\"📐 Mathematical Formulas\")\n    \n    st.markdown(\"### Energy Calculations\")\n    st.latex(r\"kWh_{year,t} = tons_{year,t} \\times yield \\times capacity\\_factor\")\n    st.latex(r\"tons_{year,t} = tpd_t \\times 365\")\n    \n    st.markdown(\"### Deterministic Forecast\")\n    st.latex(r\"tpd_t = base\\_tpd \\times (1+g)^{(t-1)}\")\n    \n    st.markdown(\"### Financial Calculations\")\n    st.latex(r\"electricity\\_revenue_t = kWh_{year,t} \\times tariff\")\n    st.latex(r\"carbon\\_revenue_t = \\frac{kWh_{year,t} \\times 0.9}{1000} \\times carbon\\_price\")\n    st.latex(r\"byproduct\\_revenue_t = tons_{year,t} \\times 0.3 \\times byproduct\\_price\")\n    st.latex(r\"total\\_revenue_t = electricity\\_revenue_t + carbon\\_revenue_t + byproduct\\_revenue_t\")\n    st.latex(r\"opex_t = tons_{year,t} \\times opex\\_per\\_ton + fixed\\_opex\")\n    st.latex(r\"ncf_t = total\\_revenue_t - opex_t\")\n    \n    st.markdown(\"### NPV Calculation\")\n    st.latex(r\"NPV = -CAPEX + \\sum_{t=1}^{horizon} \\frac{ncf_t}{(1+r)^t}\")\n    \n    st.markdown(\"### Environmental Impact\")\n    st.latex(r\"CO_2\\_savings = \\frac{kWh \\times 0.9}{1000} \\text{ tons}\")\n    st.latex(r\"Trees\\_equivalent = 50 \\times CO_2\\_savings\")\n    \n    # Parameter table\n    st.subheader(\"⚙️ Current Parameters\")\n    \n    param_data = {\n        'Parameter': [\n            'Yield Rate (kWh/ton)',\n            'Capacity Factor (%)',\n            'Tariff (₹/kWh)',\n            'OPEX per ton (₹)',\n            'Fixed OPEX (₹/year)',\n            'CAPEX (₹)',\n            'Discount Rate (%)',\n            'Project Horizon (years)',\n            'Carbon Credit Price (₹/credit)',\n            'Byproduct Sales Enabled',\n            'Byproduct Price (₹/ton)'\n        ],\n        'Value': [\n            f\"{params['yield_rate']:.1f}\",\n            f\"{params['capacity_factor']*100:.1f}%\",\n            f\"₹{params['tariff']:.2f}\",\n            f\"₹{params['opex_per_ton']:.0f}\",\n            f\"₹{params['fixed_opex']:,.0f}\",\n            f\"₹{params['capex']:,.0f}\",\n            f\"{params['discount_rate']*100:.1f}%\",\n            f\"{params['horizon_years']} years\",\n            f\"₹{params['carbon_credit_price']:.0f}\" if params['carbon_credit_price'] > 0 else \"Not used\",\n            \"Yes\" if params['enable_byproduct'] else \"No\",\n            f\"₹{params['byproduct_price']:.0f}\" if params['enable_byproduct'] else \"N/A\"\n        ],\n        'Unit': [\n            'kWh/ton',\n            'Percentage',\n            'INR per kWh',\n            'INR per ton',\n            'INR per year',\n            'INR',\n            'Percentage',\n            'Years',\n            'INR per credit',\n            'Boolean',\n            'INR per ton'\n        ]\n    }\n    \n    param_df = pd.DataFrame(param_data)\n    st.dataframe(param_df, use_container_width=True)\n    \n    # Data provenance\n    st.subheader(\"📊 Data Provenance\")\n    \n    provenance_info = {\n        \"User Data\": \"In-memory storage during session\",\n        \"Waste Logs\": \"User-entered data with auto-recorded timestamps\",\n        \"City Data\": \"Curated from government and industry sources\",\n        \"Facility Data\": \"Verified bioenergy facilities database\",\n        \"Tariff Data\": \"State electricity regulatory commission rates\",\n        \"Cost Parameters\": \"Industry benchmarks and user-adjustable\"\n    }\n    \n    for source, description in provenance_info.items():\n        st.write(f\"**{source}:** {description}\")\n    \n    # Configuration export\n    st.subheader(\"💾 Export Configuration\")\n    \n    # Create run configuration\n    run_config = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"user\": st.session_state.username,\n        \"entity\": st.session_state.user_profile.entity_name,\n        \"city\": st.session_state.user_profile.city,\n        \"waste_type\": st.session_state.user_profile.waste_type,\n        \"parameters\": params,\n        \"total_logs\": len(get_user_logs(st.session_state.username)),\n        \"application_version\": \"BIA v1.0\"\n    }\n    \n    config_json = json.dumps(run_config, indent=2, default=str)\n    \n    st.download_button(\n        label=\"Download Run Configuration\",\n        data=config_json,\n        file_name=f\"bia_config_{st.session_state.username}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\",\n        mime=\"application/json\"\n    )\n    \n    # Assumptions and limitations\n    st.subheader(\"⚠️ Assumptions & Limitations\")\n    \n    st.markdown(\"\"\"\n    **Key Assumptions:**\n    - Constant yield rate and capacity factor over project lifetime\n    - Linear waste growth for deterministic forecasting\n    - Constant tariff rates (inflation not modeled)\n    - No major policy or regulatory changes\n    - Standard CO₂ emission factor (0.9 kg/kWh)\n    \n    **Limitations:**\n    - Supports only 8 cities in current version\n    - In-memory authentication (no data persistence)\n    - Simplified financial modeling (no tax considerations)\n    - SARIMA model assumes sufficient historical data\n    - No detailed technical feasibility assessment\n    \"\"\")\n\ndef get_kpi_data(params):\n    \"\"\"Calculate KPI data for dashboard\"\"\"\n    logs = get_user_logs(st.session_state.username)\n    \n    if not logs:\n        return {\n            'total_waste': 0,\n            'total_energy': 0,\n            'co2_saved': 0\n        }\n    \n    total_waste = sum(log.waste_tons for log in logs)\n    total_energy = total_waste * params['yield_rate'] * params['capacity_factor']\n    co2_saved = (total_energy * 0.9) / 1000  # kg to tons\n    \n    return {\n        'total_waste': total_waste,\n        'total_energy': total_energy,\n        'co2_saved': co2_saved\n    }\n\ndef clean_dashboard_screen(params):\n    \"\"\"Clean first screen with KPI cards and log waste action\"\"\"\n    st.title(f\"🔋 {t('dashboard')}\")\n    st.markdown(f\"**Welcome, {st.session_state.user_profile.entity_name}!**\")\n    \n    # Get KPI data\n    kpi_data = get_kpi_data(params)\n    \n    # KPI Cards\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        st.metric(\n            label=t(\"waste_processed\"),\n            value=f\"{kpi_data['total_waste']:.1f} {t('tons')}\",\n            delta=None,\n            help=\"Total waste processed so far\"\n        )\n    \n    with col2:\n        st.metric(\n            label=t(\"energy_generated\"),\n            value=f\"{format_number(kpi_data['total_energy'])} {t('kwh')}\",\n            delta=None,\n            help=\"Total energy generated from waste\"\n        )\n    \n    with col3:\n        st.metric(\n            label=t(\"co2_saved\"),\n            value=f\"{kpi_data['co2_saved']:.1f} {t('tons')}\",\n            delta=None,\n            help=\"CO₂ emissions saved\"\n        )\n    \n    st.divider()\n    \n    # Quick waste logging\n    st.subheader(t(\"log_waste\"))\n    \n    with st.form(\"quick_waste_log\"):\n        col1, col2, col3 = st.columns([2, 2, 1])\n        \n        with col1:\n            waste_amount = st.number_input(\n                t(\"waste_amount\"),\n                min_value=0.01,\n                max_value=1000.0,\n                value=1.0,\n                step=0.1\n            )\n        \n        with col2:\n            log_date = st.date_input(t(\"date\"), value=date.today())\n        \n        with col3:\n            st.write(\"\")  # Spacer\n            st.write(\"\")  # Spacer\n            submit_btn = st.form_submit_button(\n                t(\"add_waste_log\"),\n                type=\"primary\",\n                use_container_width=True\n            )\n        \n        if submit_btn:\n            if waste_amount > 0:\n                waste_log = WasteLog(\n                    username=st.session_state.username,\n                    date=log_date,\n                    waste_tons=waste_amount\n                )\n                add_waste_log(waste_log)\n                st.success(f\"Added {waste_amount} {t('tons')} of waste for {log_date}\")\n                st.rerun()\n            else:\n                st.error(\"Please enter a valid waste amount\")\n\ndef main_dashboard():\n    \"\"\"Main dashboard with tabs\"\"\"\n    # Get parameters from sidebar\n    params = sidebar_controls()\n    \n    # Show clean dashboard first\n    clean_dashboard_screen(params)\n    \n    st.divider()\n    \n    # Create tabs for detailed analysis\n    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([\n        f\"🏢 {t('entity_profile')}\",\n        f\"📈 {t('waste_trend_forecast')}\", \n        f\"⚡ {t('energy_finance')}\",\n        f\"💹 {t('npv_sensitivity')}\",\n        f\"🗺️ {t('facilities_map')}\",\n        f\"🔍 {t('audit')}\"\n    ])\n    \n    with tab1:\n        entity_profile_tab()\n    \n    with tab2:\n        forecast_tab(params)\n    \n    with tab3:\n        energy_finance_tab(params)\n    \n    with tab4:\n        npv_sensitivity_tab(params)\n    \n    with tab5:\n        facilities_map_tab()\n    \n    with tab6:\n        audit_tab(params)\n\ndef main():\n    \"\"\"Main application\"\"\"\n    init_session_state()\n    \n    if not st.session_state.logged_in:\n        login_signup_page()\n    else:\n        main_dashboard()\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":47478},"auth_inmemory.py":{"content":"\"\"\"\nIn-memory authentication system for BIA application.\nUses bcrypt for password hashing and Streamlit cache for thread-safe storage.\n\"\"\"\n\nimport bcrypt\nimport streamlit as st\nfrom typing import Optional, List, Dict, Any\nfrom datetime import datetime, date\nfrom bia_core.schemas import UserProfile, WasteLog\n\n@st.cache_resource\ndef get_auth_store():\n    \"\"\"Get thread-safe authentication store\"\"\"\n    return {\n        'users': {},\n        'waste_logs': []\n    }\n\nclass AuthStore:\n    \"\"\"Thread-safe authentication and data store\"\"\"\n    \n    def __init__(self):\n        self.store = get_auth_store()\n        self._init_demo_user()\n    \n    def _init_demo_user(self):\n        \"\"\"Initialize demo user if not exists\"\"\"\n        if 'demo' not in self.store['users']:\n            # Hash the demo password\n            password_hash = bcrypt.hashpw('demo123'.encode('utf-8'), bcrypt.gensalt())\n            \n            demo_profile = UserProfile(\n                username='demo',\n                password_hash=password_hash.decode('utf-8'),\n                entity_name='Demo Bio-energy Corp',\n                city='Mumbai',\n                waste_type='organic'\n            )\n            \n            self.store['users']['demo'] = demo_profile\n    \n    def add_user(self, username: str, password: str, entity_name: str, \n                 city: str, waste_type: str) -> bool:\n        \"\"\"Add new user with hashed password\"\"\"\n        if username in self.store['users']:\n            return False\n        \n        # Hash password\n        password_hash = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt())\n        \n        user_profile = UserProfile(\n            username=username,\n            password_hash=password_hash.decode('utf-8'),\n            entity_name=entity_name,\n            city=city,\n            waste_type=waste_type\n        )\n        \n        self.store['users'][username] = user_profile\n        return True\n    \n    def validate_user(self, username: str, password: str) -> Optional[UserProfile]:\n        \"\"\"Validate user credentials\"\"\"\n        if username not in self.store['users']:\n            return None\n        \n        user_profile = self.store['users'][username]\n        \n        # Check password\n        if bcrypt.checkpw(password.encode('utf-8'), user_profile.password_hash.encode('utf-8')):\n            return user_profile\n        \n        return None\n    \n    def add_waste_log(self, waste_log: WasteLog):\n        \"\"\"Add waste log entry\"\"\"\n        self.store['waste_logs'].append(waste_log)\n    \n    def get_user_logs(self, username: str) -> List[WasteLog]:\n        \"\"\"Get all waste logs for a user\"\"\"\n        return [log for log in self.store['waste_logs'] if log.username == username]\n\n# Global auth store instance\nauth_store = AuthStore()\n\ndef add_user(username: str, password: str, entity_name: str, \n             city: str, waste_type: str) -> bool:\n    \"\"\"Add new user\"\"\"\n    return auth_store.add_user(username, password, entity_name, city, waste_type)\n\ndef validate_user(username: str, password: str) -> Optional[UserProfile]:\n    \"\"\"Validate user credentials\"\"\"\n    return auth_store.validate_user(username, password)\n\ndef add_waste_log(waste_log: WasteLog):\n    \"\"\"Add waste log\"\"\"\n    auth_store.add_waste_log(waste_log)\n\ndef get_user_logs(username: str) -> List[WasteLog]:\n    \"\"\"Get user's waste logs\"\"\"\n    return auth_store.get_user_logs(username)\n","size_bytes":3384},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"folium>=0.20.0\",\n    \"matplotlib>=3.10.5\",\n    \"numpy>=2.3.2\",\n    \"plotly>=6.3.0\",\n    \"pydantic>=2.11.7\",\n    \"pydeck>=0.9.1\",\n    \"scikit-learn>=1.7.1\",\n    \"statsmodels>=0.14.5\",\n    \"streamlit-folium>=0.25.1\",\n    \"streamlit>=1.48.1\",\n    \"python-dateutil>=2.9.0.post0\",\n    \"bcrypt>=4.3.0\",\n    \"pyyaml>=6.0.2\",\n    \"pandas>=2.3.2\",\n    \"sqlalchemy>=2.0.43\",\n    \"psycopg2-binary>=2.9.10\",\n]\n","size_bytes":545},"replit.md":{"content":"# BIA - Bio-energy Intelligence Application\n\n## Overview\n\nBIA (Bio-energy Intelligence Application) is a comprehensive Streamlit-based platform for bio-energy intelligence and waste-to-energy analysis. The application provides end-to-end capabilities for tracking waste generation, forecasting energy potential, performing financial analysis, and visualizing bioenergy facilities across major Indian cities. It serves as a decision-support tool for bioenergy stakeholders, enabling data-driven investment and operational decisions through integrated waste logging, predictive modeling, and financial evaluation capabilities.\n\n## User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n## System Architecture\n\n### Frontend Architecture\n- **Framework**: Streamlit-based web application with reactive UI components\n- **Layout**: Wide layout with expandable sidebar for navigation\n- **State Management**: Session-based state management using Streamlit's built-in session state\n- **Visualization**: Plotly for interactive charts and graphs, Folium for mapping functionality\n- **User Interface**: Multi-page application structure with authentication, dashboard, forecasting, financial analysis, and mapping sections\n\n### Backend Architecture\n- **Core Module Structure**: Modular architecture with `bia_core` package containing business logic\n- **Data Processing**: Pandas-based data manipulation and feature engineering\n- **Authentication**: In-memory authentication using bcrypt for password hashing with thread-safe storage via Streamlit cache\n- **Models**: Pluggable forecasting model architecture supporting deterministic and SARIMA models\n- **Financial Engine**: Comprehensive financial calculator for NPV, ROI, and cashflow analysis\n- **Validation**: Pydantic schemas for data validation and type safety\n\n### Data Storage Solutions\n- **Primary Storage**: Optional Supabase/PostgreSQL database or in-memory storage using Streamlit's `@st.cache_resource`\n- **Database Backend**: Configurable persistence layer with automatic fallback to in-memory storage\n- **User Data**: Thread-safe authentication store managing user profiles and waste logs with bcrypt password hashing\n- **Curated Data**: CSV-based data storage for city statistics, facilities, tariffs, and cost parameters\n- **Session Management**: Browser session-based state management for user authentication and application state\n- **Migration System**: Automatic database table creation and schema management on startup\n\n### Authentication and Authorization\n- **Password Security**: bcrypt hashing algorithm for secure password storage\n- **Session Management**: Streamlit session state for user authentication persistence\n- **User Profiles**: Comprehensive user profile system with entity information and waste type classification\n- **Demo Access**: Pre-configured demo user (demo/demo123) for immediate application testing\n\n### Key Design Patterns\n- **Model Abstraction**: Base model class with standardized fit/predict interface for forecasting models\n- **Factory Pattern**: Model selector for choosing optimal forecasting approach based on data characteristics\n- **Separation of Concerns**: Clear separation between data models, business logic, and presentation layers\n- **Error Handling**: Comprehensive error handling with graceful fallbacks for missing dependencies\n\n### Database Integration\n- **Supabase Support**: Optional PostgreSQL database persistence via DATABASE_URL environment variable\n- **Conditional Storage**: USE_SUPABASE flag automatically detects DATABASE_URL presence and enables database backend\n- **SQLAlchemy ORM**: Database operations using SQLAlchemy with psycopg2 for PostgreSQL connectivity\n- **Migration Support**: Automatic table creation for users and waste_logs with proper indexing and foreign keys\n- **Graceful Fallback**: Automatic fallback to in-memory storage if database connection fails\n\n## External Dependencies\n\n### Core Framework Dependencies\n- **Streamlit**: Primary web application framework for UI and deployment\n- **Pandas**: Data manipulation and analysis for time series processing\n- **NumPy**: Numerical computations for financial calculations and forecasting\n- **Plotly**: Interactive visualization library for charts, graphs, and financial analysis displays\n\n### Forecasting and Analytics\n- **Statsmodels**: Advanced time series modeling including SARIMA models (optional dependency with fallback)\n- **Scikit-learn**: Machine learning utilities for model evaluation and data preprocessing\n- **Python-dateutil**: Date parsing and manipulation for time series analysis\n\n### Mapping and Visualization\n- **Folium**: Interactive map generation for facility mapping\n- **Streamlit-folium**: Streamlit integration for Folium maps\n- **Pydeck**: Advanced geospatial visualization capabilities\n- **Matplotlib**: Additional plotting capabilities for specialized visualizations\n\n### Security and Data Validation\n- **Bcrypt**: Cryptographic password hashing for user authentication\n- **Pydantic**: Data validation and settings management with type annotations\n- **PyYAML**: Configuration file parsing for application settings\n\n### Geographic Coverage\n- **Supported Cities**: Ahmedabad, Gandhinagar, Indore, Delhi, Mumbai, Pune, Bengaluru, Chennai\n- **Waste Types**: Organic, industrial, and agricultural waste categories\n- **Map Data**: Curated facility database with geographic coordinates and operational status\n\n### Data Sources and Integrations\n- **City Statistics**: Population, waste generation rates, and waste composition data\n- **Facility Database**: Bioenergy facility locations, capacities, and operational status\n- **Tariff Data**: Electricity pricing information for financial calculations\n- **Cost Parameters**: Capital and operational expenditure benchmarks for project evaluation","size_bytes":5811},"supabase_store.py":{"content":"\"\"\"\nSupabase/PostgreSQL persistence layer for BIA application.\nUses SQLAlchemy with DATABASE_URL from environment secrets.\n\"\"\"\n\nimport os\nimport logging\nfrom datetime import datetime, date\nfrom typing import List, Optional\nfrom sqlalchemy import create_engine, text, Column, String, Float, Date, DateTime, Boolean, Integer\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom bia_core.schemas import UserProfile, WasteLog\nimport bcrypt\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nBase = declarative_base()\n\nclass User(Base):\n    __tablename__ = 'users'\n    \n    username = Column(String(50), primary_key=True)\n    password_hash = Column(String(128), nullable=False)\n    entity_name = Column(String(100), nullable=False)\n    city = Column(String(50), nullable=False)\n    waste_type = Column(String(50), nullable=False)\n    created_at = Column(DateTime, default=datetime.utcnow)\n\nclass WasteLogEntry(Base):\n    __tablename__ = 'waste_logs'\n    \n    id = Column(String(50), primary_key=True)  # Will use UUID or composite key\n    username = Column(String(50), nullable=False)\n    date = Column(Date, nullable=False)\n    waste_tons = Column(Float, nullable=False)\n    notes = Column(String(500), default=\"\")\n    created_at = Column(DateTime, default=datetime.utcnow)\n\n# Database connection\nDATABASE_URL = os.environ.get('DATABASE_URL')\nif not DATABASE_URL:\n    raise ValueError(\"DATABASE_URL environment variable is required\")\n\n# Create engine and session\nengine = create_engine(DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\ndef get_db() -> Session:\n    \"\"\"Get database session\"\"\"\n    db = SessionLocal()\n    try:\n        return db\n    except Exception as e:\n        db.close()\n        raise e\n\ndef migrate():\n    \"\"\"Create tables if they don't exist\"\"\"\n    try:\n        logger.info(\"Running database migrations...\")\n        \n        # Create tables\n        Base.metadata.create_all(bind=engine)\n        \n        # Verify tables exist\n        with engine.connect() as conn:\n            result = conn.execute(text(\"\"\"\n                SELECT table_name \n                FROM information_schema.tables \n                WHERE table_schema = 'public' \n                AND table_name IN ('users', 'waste_logs')\n            \"\"\"))\n            tables = [row[0] for row in result]\n            \n        if 'users' in tables and 'waste_logs' in tables:\n            logger.info(\"Database migration completed successfully\")\n            return True\n        else:\n            logger.error(f\"Migration failed. Found tables: {tables}\")\n            return False\n            \n    except Exception as e:\n        logger.error(f\"Migration error: {e}\")\n        return False\n\ndef add_user(username: str, password: str, entity_name: str, city: str, waste_type: str) -> bool:\n    \"\"\"Add a new user to the database\"\"\"\n    try:\n        db = get_db()\n        \n        # Check if user already exists\n        existing_user = db.query(User).filter(User.username == username).first()\n        if existing_user:\n            db.close()\n            return False\n        \n        # Hash password\n        password_hash = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt()).decode('utf-8')\n        \n        # Create new user\n        new_user = User()\n        setattr(new_user, 'username', username)\n        setattr(new_user, 'password_hash', password_hash)\n        setattr(new_user, 'entity_name', entity_name)\n        setattr(new_user, 'city', city)\n        setattr(new_user, 'waste_type', waste_type)\n        \n        db.add(new_user)\n        db.commit()\n        db.close()\n        \n        logger.info(f\"User {username} added successfully\")\n        return True\n        \n    except Exception as e:\n        logger.error(f\"Error adding user {username}: {e}\")\n        return False\n\ndef validate_user(username: str, password: str) -> Optional[UserProfile]:\n    \"\"\"Validate user credentials and return user profile\"\"\"\n    try:\n        db = get_db()\n        \n        user = db.query(User).filter(User.username == username).first()\n        if not user:\n            db.close()\n            return None\n        \n        # Verify password\n        if bcrypt.checkpw(password.encode('utf-8'), getattr(user, 'password_hash').encode('utf-8')):\n            profile = UserProfile(\n                username=getattr(user, 'username'),\n                password_hash='[PROTECTED]',  # Don't expose real hash\n                entity_name=getattr(user, 'entity_name'),\n                city=getattr(user, 'city'),\n                waste_type=getattr(user, 'waste_type').lower()  # Ensure lowercase\n            )\n            db.close()\n            return profile\n        \n        db.close()\n        return None\n        \n    except Exception as e:\n        logger.error(f\"Error validating user {username}: {e}\")\n        return None\n\ndef add_waste_log(username: str, log_date: date, waste_tons: float, notes: str = \"\") -> bool:\n    \"\"\"Add a waste log entry\"\"\"\n    try:\n        db = get_db()\n        \n        # Create unique ID (username + date)\n        log_id = f\"{username}_{log_date.isoformat()}\"\n        \n        # Check if log already exists for this date\n        existing_log = db.query(WasteLogEntry).filter(\n            WasteLogEntry.username == username,\n            WasteLogEntry.date == log_date\n        ).first()\n        \n        if existing_log:\n            # Update existing log  \n            db.query(WasteLogEntry).filter(\n                WasteLogEntry.username == username,\n                WasteLogEntry.date == log_date\n            ).update({\n                'waste_tons': waste_tons,\n                'notes': notes\n            })\n            logger.info(f\"Updated waste log for {username} on {log_date}\")\n        else:\n            # Create new log\n            new_log = WasteLogEntry()\n            setattr(new_log, 'id', log_id)\n            setattr(new_log, 'username', username)\n            setattr(new_log, 'date', log_date)\n            setattr(new_log, 'waste_tons', waste_tons)\n            setattr(new_log, 'notes', notes)\n            db.add(new_log)\n            logger.info(f\"Added new waste log for {username} on {log_date}\")\n        \n        db.commit()\n        db.close()\n        return True\n        \n    except Exception as e:\n        logger.error(f\"Error adding waste log for {username}: {e}\")\n        return False\n\ndef get_user_logs(username: str) -> List[WasteLog]:\n    \"\"\"Get all waste logs for a user\"\"\"\n    try:\n        db = get_db()\n        \n        logs = db.query(WasteLogEntry).filter(\n            WasteLogEntry.username == username\n        ).order_by(WasteLogEntry.date.desc()).all()\n        \n        result = []\n        for log in logs:\n            waste_log = WasteLog(\n                username=getattr(log, 'username'),\n                date=getattr(log, 'date'),\n                waste_tons=getattr(log, 'waste_tons'),\n                notes=getattr(log, 'notes', '') or \"\"\n            )\n            result.append(waste_log)\n        \n        db.close()\n        logger.info(f\"Retrieved {len(result)} logs for {username}\")\n        return result\n        \n    except Exception as e:\n        logger.error(f\"Error getting logs for {username}: {e}\")\n        return []\n\n# SQL for manual table creation (if needed)\nCREATE_TABLES_SQL = \"\"\"\n-- Users table\nCREATE TABLE IF NOT EXISTS users (\n    username VARCHAR(50) PRIMARY KEY,\n    password_hash VARCHAR(128) NOT NULL,\n    entity_name VARCHAR(100) NOT NULL,\n    city VARCHAR(50) NOT NULL,\n    waste_type VARCHAR(50) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Waste logs table\nCREATE TABLE IF NOT EXISTS waste_logs (\n    id VARCHAR(50) PRIMARY KEY,\n    username VARCHAR(50) NOT NULL,\n    date DATE NOT NULL,\n    waste_tons REAL NOT NULL,\n    notes VARCHAR(500),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (username) REFERENCES users(username) ON DELETE CASCADE,\n    UNIQUE(username, date)\n);\n\n-- Indexes for better performance\nCREATE INDEX IF NOT EXISTS idx_waste_logs_username ON waste_logs(username);\nCREATE INDEX IF NOT EXISTS idx_waste_logs_date ON waste_logs(date);\n\"\"\"\n\nif __name__ == \"__main__\":\n    # Test migration\n    if migrate():\n        print(\"Migration successful!\")\n    else:\n        print(\"Migration failed!\")","size_bytes":8343},"bia_core/__init__.py":{"content":"\"\"\"\nBIA Core - Bio-energy Intelligence Application Core Module\n\nThis module contains the core business logic for the BIA application including:\n- Data models and schemas\n- Forecasting models\n- Financial calculations\n- Mapping functionality\n- Utility functions\n\"\"\"\n\n__version__ = \"1.0.0\"\n__author__ = \"BIA Development Team\"\n\n# Core constants\nINR_CRORE = 1e7\nCO2_PER_KWH_KG = 0.9\nSUPPORTED_CITIES = [\n    \"Ahmedabad\", \"Gandhinagar\", \"Indore\", \"Delhi\", \n    \"Mumbai\", \"Pune\", \"Bengaluru\", \"Chennai\"\n]\nWASTE_TYPES = [\"organic\", \"industrial\", \"agricultural\"]\n","size_bytes":554},"bia_core/data_io.py":{"content":"\"\"\"\nData input/output operations for BIA application.\nHandles loading and processing of curated datasets.\n\"\"\"\n\nimport pandas as pd\nimport os\nfrom typing import Dict, Any\nimport streamlit as st\n\n@st.cache_data\ndef load_curated_data() -> Dict[str, pd.DataFrame]:\n    \"\"\"Load all curated data files\"\"\"\n    \n    data = {}\n    \n    # Define data files\n    data_files = {\n        'city_stats': 'data/curated/city_stats.csv',\n        'facilities': 'data/curated/facilities.csv',\n        'tariffs': 'data/curated/tariffs.csv',\n        'costs': 'data/curated/costs.csv'\n    }\n    \n    for key, filepath in data_files.items():\n        try:\n            if os.path.exists(filepath):\n                data[key] = pd.read_csv(filepath)\n            else:\n                # Create empty dataframe with expected structure\n                data[key] = create_empty_dataframe(key)\n        except Exception as e:\n            st.warning(f\"Could not load {filepath}: {str(e)}\")\n            data[key] = create_empty_dataframe(key)\n    \n    return data\n\ndef create_empty_dataframe(data_type: str) -> pd.DataFrame:\n    \"\"\"Create empty dataframe with expected structure\"\"\"\n    \n    if data_type == 'city_stats':\n        return pd.DataFrame(columns=[\n            'city', 'population', 'waste_generation_tpd', \n            'organic_fraction', 'industrial_fraction', 'agricultural_fraction'\n        ])\n    \n    elif data_type == 'facilities':\n        return pd.DataFrame(columns=[\n            'name', 'city', 'state', 'type', 'capacity_mw', \n            'status', 'lat', 'lon', 'source'\n        ])\n    \n    elif data_type == 'tariffs':\n        return pd.DataFrame(columns=[\n            'city', 'state', 'tariff_residential', 'tariff_commercial', \n            'tariff_industrial', 'renewable_tariff'\n        ])\n    \n    elif data_type == 'costs':\n        return pd.DataFrame(columns=[\n            'technology', 'capex_per_mw', 'opex_per_mwh', \n            'capacity_factor', 'lifetime_years'\n        ])\n    \n    else:\n        return pd.DataFrame()\n\ndef get_city_data(city: str) -> Dict[str, Any]:\n    \"\"\"Get data specific to a city\"\"\"\n    \n    curated_data = load_curated_data()\n    \n    city_info = {\n        'city': city,\n        'population': None,\n        'waste_generation_tpd': None,\n        'tariff': 4.5,  # Default tariff\n        'facilities_count': 0\n    }\n    \n    # Get city statistics\n    city_stats = curated_data['city_stats']\n    if not city_stats.empty:\n        city_row = city_stats[city_stats['city'] == city]\n        if not city_row.empty:\n            city_info.update(city_row.iloc[0].to_dict())\n    \n    # Get tariff information\n    tariffs = curated_data['tariffs']\n    if not tariffs.empty:\n        tariff_row = tariffs[tariffs['city'] == city]\n        if not tariff_row.empty:\n            city_info['tariff'] = tariff_row.iloc[0].get('renewable_tariff', 4.5)\n    \n    # Count facilities\n    facilities = curated_data['facilities']\n    if not facilities.empty:\n        city_facilities = facilities[facilities['city'] == city]\n        city_info['facilities_count'] = len(city_facilities)\n    \n    return city_info\n\ndef export_user_data(username: str, data: Dict[str, Any]) -> str:\n    \"\"\"Export user data to JSON format\"\"\"\n    import json\n    from datetime import datetime\n    \n    export_data = {\n        'username': username,\n        'export_timestamp': datetime.now().isoformat(),\n        'data': data\n    }\n    \n    return json.dumps(export_data, indent=2, default=str)\n","size_bytes":3465},"bia_core/eval.py":{"content":"\"\"\"\nModel evaluation functions for forecasting models.\nIncludes backtesting and performance metrics.\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Dict, Any, Tuple\nfrom bia_core.models import BaseModel\n\ndef calculate_mape(actual: List[float], predicted: List[float]) -> float:\n    \"\"\"Calculate Mean Absolute Percentage Error\"\"\"\n    \n    if len(actual) != len(predicted):\n        raise ValueError(\"Actual and predicted arrays must have same length\")\n    \n    if len(actual) == 0:\n        return float('inf')\n    \n    # Handle zeros in actual values\n    actual_array = np.array(actual)\n    predicted_array = np.array(predicted)\n    \n    # Only consider non-zero actual values\n    non_zero_mask = actual_array != 0\n    \n    if not non_zero_mask.any():\n        # If all actual values are zero, use MAE instead\n        return calculate_mae(actual, predicted)\n    \n    # Calculate MAPE only for non-zero values\n    actual_nz = actual_array[non_zero_mask]\n    predicted_nz = predicted_array[non_zero_mask]\n    \n    mape = np.mean(np.abs((actual_nz - predicted_nz) / actual_nz)) * 100\n    \n    return mape\n\ndef calculate_mae(actual: List[float], predicted: List[float]) -> float:\n    \"\"\"Calculate Mean Absolute Error\"\"\"\n    \n    if len(actual) != len(predicted):\n        raise ValueError(\"Actual and predicted arrays must have same length\")\n    \n    actual_array = np.array(actual)\n    predicted_array = np.array(predicted)\n    \n    mae = np.mean(np.abs(actual_array - predicted_array))\n    \n    return mae\n\ndef calculate_rmse(actual: List[float], predicted: List[float]) -> float:\n    \"\"\"Calculate Root Mean Square Error\"\"\"\n    \n    if len(actual) != len(predicted):\n        raise ValueError(\"Actual and predicted arrays must have same length\")\n    \n    actual_array = np.array(actual)\n    predicted_array = np.array(predicted)\n    \n    mse = np.mean((actual_array - predicted_array) ** 2)\n    rmse = np.sqrt(mse)\n    \n    return rmse\n\ndef calculate_r2(actual: List[float], predicted: List[float]) -> float:\n    \"\"\"Calculate R-squared coefficient\"\"\"\n    \n    if len(actual) != len(predicted):\n        raise ValueError(\"Actual and predicted arrays must have same length\")\n    \n    actual_array = np.array(actual)\n    predicted_array = np.array(predicted)\n    \n    # Calculate R-squared\n    ss_res = np.sum((actual_array - predicted_array) ** 2)\n    ss_tot = np.sum((actual_array - np.mean(actual_array)) ** 2)\n    \n    if ss_tot == 0:\n        return 0.0  # Perfect fit if no variance in actual\n    \n    r2 = 1 - (ss_res / ss_tot)\n    \n    return r2\n\ndef backtest_model(model: BaseModel, features_df: pd.DataFrame, \n                   test_size: int = 7, min_train_size: int = 7) -> float:\n    \"\"\"\n    Perform time series cross-validation backtesting\n    \n    Args:\n        model: Fitted forecasting model\n        features_df: Historical data\n        test_size: Number of days to forecast in each test\n        min_train_size: Minimum training size\n    \n    Returns:\n        MAPE score from backtesting\n    \"\"\"\n    \n    if len(features_df) < min_train_size + test_size:\n        return float('inf')\n    \n    all_actuals = []\n    all_predictions = []\n    \n    # Sliding window backtesting\n    max_train_end = len(features_df) - test_size\n    \n    for train_end in range(min_train_size, max_train_end + 1, test_size):\n        # Split data\n        train_data = features_df.iloc[:train_end].copy()\n        test_data = features_df.iloc[train_end:train_end + test_size].copy()\n        \n        if len(test_data) == 0:\n            continue\n        \n        try:\n            # Fit model on training data\n            model.fit(train_data)\n            \n            # Generate forecast\n            forecast = model.predict(len(test_data))\n            \n            # Collect results\n            actual_values = test_data['waste_tons'].tolist()\n            all_actuals.extend(actual_values)\n            all_predictions.extend(forecast[:len(actual_values)])\n            \n        except Exception as e:\n            print(f\"Backtest iteration failed: {e}\")\n            continue\n    \n    if len(all_actuals) == 0:\n        return float('inf')\n    \n    # Calculate MAPE\n    mape = calculate_mape(all_actuals, all_predictions)\n    \n    return mape\n\ndef evaluate_model_performance(model: BaseModel, features_df: pd.DataFrame,\n                              test_split: float = 0.3) -> Dict[str, float]:\n    \"\"\"\n    Comprehensive model evaluation\n    \n    Args:\n        model: Forecasting model\n        features_df: Historical data\n        test_split: Fraction of data for testing\n    \n    Returns:\n        Dictionary of performance metrics\n    \"\"\"\n    \n    if len(features_df) < 10:\n        return {\n            'mape': float('inf'),\n            'mae': float('inf'),\n            'rmse': float('inf'),\n            'r2': 0.0,\n            'data_points': len(features_df)\n        }\n    \n    # Split data\n    split_idx = int(len(features_df) * (1 - test_split))\n    train_data = features_df.iloc[:split_idx].copy()\n    test_data = features_df.iloc[split_idx:].copy()\n    \n    if len(test_data) == 0:\n        return {\n            'mape': float('inf'),\n            'mae': float('inf'),\n            'rmse': float('inf'),\n            'r2': 0.0,\n            'data_points': len(features_df)\n        }\n    \n    try:\n        # Fit model\n        model.fit(train_data)\n        \n        # Generate predictions\n        forecast = model.predict(len(test_data))\n        actual = test_data['waste_tons'].tolist()\n        \n        # Calculate metrics\n        mape = calculate_mape(actual, forecast)\n        mae = calculate_mae(actual, forecast)\n        rmse = calculate_rmse(actual, forecast)\n        r2 = calculate_r2(actual, forecast)\n        \n        return {\n            'mape': mape,\n            'mae': mae,\n            'rmse': rmse,\n            'r2': r2,\n            'data_points': len(features_df),\n            'test_points': len(test_data)\n        }\n        \n    except Exception as e:\n        print(f\"Model evaluation failed: {e}\")\n        return {\n            'mape': float('inf'),\n            'mae': float('inf'),\n            'rmse': float('inf'),\n            'r2': 0.0,\n            'data_points': len(features_df)\n        }\n\ndef compare_models(models: List[BaseModel], features_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Compare performance of multiple models\"\"\"\n    \n    results = []\n    \n    for model in models:\n        model_name = type(model).__name__\n        \n        try:\n            # Evaluate model\n            performance = evaluate_model_performance(model, features_df)\n            \n            # Add model info\n            performance['model'] = model_name\n            results.append(performance)\n            \n        except Exception as e:\n            print(f\"Failed to evaluate {model_name}: {e}\")\n            results.append({\n                'model': model_name,\n                'mape': float('inf'),\n                'mae': float('inf'),\n                'rmse': float('inf'),\n                'r2': 0.0,\n                'data_points': len(features_df)\n            })\n    \n    # Create comparison dataframe\n    comparison_df = pd.DataFrame(results)\n    \n    if not comparison_df.empty:\n        comparison_df = comparison_df.sort_values('mape')\n    \n    return comparison_df\n\ndef forecast_confidence_interval(model: BaseModel, features_df: pd.DataFrame,\n                               forecast_days: int, confidence_level: float = 0.95) -> Tuple[List[float], List[float], List[float]]:\n    \"\"\"\n    Generate forecast with confidence intervals (simplified approach)\n    \n    Args:\n        model: Fitted model\n        features_df: Historical data\n        forecast_days: Number of days to forecast\n        confidence_level: Confidence level (e.g., 0.95 for 95%)\n    \n    Returns:\n        Tuple of (forecast, lower_bound, upper_bound)\n    \"\"\"\n    \n    # Fit model\n    model.fit(features_df)\n    \n    # Generate base forecast\n    forecast = model.predict(forecast_days)\n    \n    # Calculate residuals for confidence interval estimation\n    if len(features_df) >= 10:\n        # Use recent data for residual calculation\n        recent_data = features_df.tail(min(30, len(features_df)))\n        \n        # Simple residual-based confidence intervals\n        predictions_for_residuals = []\n        actuals_for_residuals = recent_data['waste_tons'].tolist()\n        \n        # Generate predictions for recent period\n        for i in range(len(recent_data)):\n            temp_train = recent_data.iloc[:max(1, i)].copy()\n            if len(temp_train) > 0:\n                temp_model = type(model)()\n                temp_model.fit(temp_train)\n                pred = temp_model.predict(1)\n                predictions_for_residuals.append(pred[0] if pred else 0)\n            else:\n                predictions_for_residuals.append(0)\n        \n        # Calculate residual standard deviation\n        residuals = np.array(actuals_for_residuals) - np.array(predictions_for_residuals)\n        residual_std = np.std(residuals)\n        \n        # Z-score for confidence level\n        from scipy import stats\n        z_score = stats.norm.ppf((1 + confidence_level) / 2)\n        \n        # Calculate bounds\n        margin = z_score * residual_std\n        lower_bound = [max(0, f - margin) for f in forecast]\n        upper_bound = [f + margin for f in forecast]\n        \n    else:\n        # Use simple percentage bounds if insufficient data\n        margin_pct = 0.2  # 20% margin\n        lower_bound = [max(0, f * (1 - margin_pct)) for f in forecast]\n        upper_bound = [f * (1 + margin_pct) for f in forecast]\n    \n    return forecast, lower_bound, upper_bound\n\ndef residual_analysis(model: BaseModel, features_df: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"Perform residual analysis for model diagnostics\"\"\"\n    \n    if len(features_df) < 10:\n        return {'error': 'Insufficient data for residual analysis'}\n    \n    # Split data for out-of-sample residuals\n    split_idx = int(len(features_df) * 0.7)\n    train_data = features_df.iloc[:split_idx].copy()\n    test_data = features_df.iloc[split_idx:].copy()\n    \n    try:\n        # Fit model\n        model.fit(train_data)\n        \n        # Generate predictions\n        predictions = model.predict(len(test_data))\n        actuals = test_data['waste_tons'].tolist()\n        \n        # Calculate residuals\n        residuals = np.array(actuals) - np.array(predictions)\n        \n        # Residual statistics\n        residual_stats = {\n            'mean': np.mean(residuals),\n            'std': np.std(residuals),\n            'min': np.min(residuals),\n            'max': np.max(residuals),\n            'median': np.median(residuals),\n            'skewness': float(np.mean(((residuals - np.mean(residuals)) / np.std(residuals)) ** 3)),\n            'kurtosis': float(np.mean(((residuals - np.mean(residuals)) / np.std(residuals)) ** 4))\n        }\n        \n        return {\n            'residual_stats': residual_stats,\n            'residuals': residuals.tolist(),\n            'predictions': predictions,\n            'actuals': actuals\n        }\n        \n    except Exception as e:\n        return {'error': f'Residual analysis failed: {e}'}\n","size_bytes":11154},"bia_core/features.py":{"content":"\"\"\"\nFeature engineering for forecasting models.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom typing import Dict, Any, List\nfrom datetime import datetime, timedelta\n\ndef create_forecast_features(df_logs: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Create features for forecasting models\"\"\"\n    \n    if df_logs.empty:\n        return pd.DataFrame()\n    \n    # Ensure date column is datetime\n    df = df_logs.copy()\n    if 'date' in df.columns:\n        df['date'] = pd.to_datetime(df['date'])\n    \n    # Sort by date\n    df = df.sort_values('date').reset_index(drop=True)\n    \n    # Aggregate by date (in case multiple logs per day)\n    daily_data = df.groupby('date')['waste_tons'].sum().reset_index()\n    \n    # Fill missing dates\n    if len(daily_data) > 1:\n        date_range = pd.date_range(\n            start=daily_data['date'].min(),\n            end=daily_data['date'].max(),\n            freq='D'\n        )\n        \n        full_df = pd.DataFrame({'date': date_range})\n        daily_data = full_df.merge(daily_data, on='date', how='left')\n        daily_data['waste_tons'] = daily_data['waste_tons'].fillna(0)\n    \n    # Create time-based features\n    daily_data['day_of_week'] = daily_data['date'].dt.dayofweek\n    daily_data['day_of_month'] = daily_data['date'].dt.day\n    daily_data['month'] = daily_data['date'].dt.month\n    daily_data['quarter'] = daily_data['date'].dt.quarter\n    \n    # Create lag features\n    for lag in [1, 7, 30]:\n        if len(daily_data) > lag:\n            daily_data[f'waste_lag_{lag}'] = daily_data['waste_tons'].shift(lag)\n    \n    # Rolling averages\n    for window in [7, 14, 30]:\n        if len(daily_data) >= window:\n            daily_data[f'waste_ma_{window}'] = daily_data['waste_tons'].rolling(\n                window=window, min_periods=1\n            ).mean()\n    \n    # Trend features\n    daily_data['days_since_start'] = (\n        daily_data['date'] - daily_data['date'].min()\n    ).dt.days\n    \n    # Growth rate (if enough data)\n    if len(daily_data) > 7:\n        daily_data['growth_rate_7d'] = (\n            daily_data['waste_tons'] / daily_data['waste_tons'].shift(7) - 1\n        ).fillna(0)\n    \n    # Cumulative features\n    daily_data['cumulative_waste'] = daily_data['waste_tons'].cumsum()\n    \n    # Seasonality indicators\n    daily_data['is_weekend'] = daily_data['day_of_week'].isin([5, 6]).astype(int)\n    daily_data['is_month_start'] = (daily_data['day_of_month'] <= 5).astype(int)\n    daily_data['is_month_end'] = (daily_data['day_of_month'] >= 25).astype(int)\n    \n    return daily_data\n\ndef prepare_sarima_data(features_df: pd.DataFrame) -> pd.Series:\n    \"\"\"Prepare data for SARIMA modeling\"\"\"\n    \n    if features_df.empty:\n        return pd.Series()\n    \n    # Create time series with date index\n    ts_data = features_df.set_index('date')['waste_tons']\n    \n    # Handle missing values\n    ts_data = ts_data.fillna(method='ffill').fillna(0)\n    \n    return ts_data\n\ndef calculate_baseline_growth(features_df: pd.DataFrame) -> float:\n    \"\"\"Calculate baseline growth rate for deterministic model\"\"\"\n    \n    if len(features_df) < 7:\n        return 0.02  # Default 2% growth\n    \n    # Calculate growth from first week to last week\n    first_week_avg = features_df['waste_tons'][:7].mean()\n    last_week_avg = features_df['waste_tons'][-7:].mean()\n    \n    if first_week_avg > 0:\n        total_days = len(features_df)\n        total_growth = (last_week_avg / first_week_avg) - 1\n        daily_growth = (1 + total_growth) ** (1/total_days) - 1\n        \n        # Cap growth rate to reasonable bounds\n        daily_growth = max(-0.01, min(0.01, daily_growth))\n        \n        return daily_growth\n    \n    return 0.002  # Default 0.2% daily growth\n\ndef extract_seasonality_patterns(features_df: pd.DataFrame) -> Dict[str, float]:\n    \"\"\"Extract seasonality patterns from historical data\"\"\"\n    \n    patterns = {\n        'weekend_factor': 1.0,\n        'month_start_factor': 1.0,\n        'month_end_factor': 1.0,\n        'quarterly_factors': [1.0, 1.0, 1.0, 1.0]\n    }\n    \n    if len(features_df) < 30:\n        return patterns\n    \n    # Weekend vs weekday pattern\n    weekend_avg = features_df[features_df['is_weekend'] == 1]['waste_tons'].mean()\n    weekday_avg = features_df[features_df['is_weekend'] == 0]['waste_tons'].mean()\n    \n    if weekday_avg > 0:\n        patterns['weekend_factor'] = weekend_avg / weekday_avg\n    \n    # Month start/end patterns\n    month_start_avg = features_df[features_df['is_month_start'] == 1]['waste_tons'].mean()\n    month_end_avg = features_df[features_df['is_month_end'] == 1]['waste_tons'].mean()\n    overall_avg = features_df['waste_tons'].mean()\n    \n    if overall_avg > 0:\n        patterns['month_start_factor'] = month_start_avg / overall_avg\n        patterns['month_end_factor'] = month_end_avg / overall_avg\n    \n    # Quarterly patterns\n    for quarter in range(1, 5):\n        quarter_data = features_df[features_df['quarter'] == quarter]['waste_tons']\n        if len(quarter_data) > 0 and overall_avg > 0:\n            patterns['quarterly_factors'][quarter-1] = quarter_data.mean() / overall_avg\n    \n    return patterns\n\ndef create_forecast_dates(last_date: pd.Timestamp, forecast_days: int) -> pd.DatetimeIndex:\n    \"\"\"Create forecast date range\"\"\"\n    \n    start_date = last_date + pd.Timedelta(days=1)\n    end_date = start_date + pd.Timedelta(days=forecast_days-1)\n    \n    return pd.date_range(start=start_date, end=end_date, freq='D')\n\ndef validate_forecast_inputs(features_df: pd.DataFrame, forecast_days: int) -> bool:\n    \"\"\"Validate inputs for forecasting\"\"\"\n    \n    if features_df.empty:\n        return False\n    \n    if forecast_days < 1 or forecast_days > 365:\n        return False\n    \n    if 'waste_tons' not in features_df.columns:\n        return False\n    \n    if features_df['waste_tons'].isna().all():\n        return False\n    \n    return True\n","size_bytes":5869},"bia_core/finance.py":{"content":"\"\"\"\nFinancial calculations for bioenergy projects.\nIncludes NPV, payback, ROI, and cashflow analysis.\n\"\"\"\n\nimport numpy as np\nfrom typing import List, Dict, Any, Tuple\nfrom bia_core.schemas import FinancialParameters, CashflowItem, NPVResults\nfrom bia_core import INR_CRORE, CO2_PER_KWH_KG\n\nclass FinanceCalculator:\n    \"\"\"Financial calculator for bioenergy projects\"\"\"\n    \n    def __init__(self, yield_rate: float, capacity_factor: float, tariff: float,\n                 opex_per_ton: float, fixed_opex: float, capex: float, \n                 discount_rate: float, carbon_credit_price: float = 0.0, \n                 byproduct_price: float = 0.0, enable_byproduct: bool = False):\n        \"\"\"\n        Initialize finance calculator\n        \n        Args:\n            yield_rate: Energy yield in kWh/ton\n            capacity_factor: Plant capacity factor (0-1)\n            tariff: Electricity tariff in ₹/kWh\n            opex_per_ton: Variable OPEX in ₹/ton\n            fixed_opex: Fixed OPEX in ₹/year\n            capex: Capital expenditure in ₹\n            discount_rate: Discount rate (0-1)\n            carbon_credit_price: Price per carbon credit in ₹\n            byproduct_price: Price per ton of byproduct in ₹\n            enable_byproduct: Whether to include byproduct revenue\n        \"\"\"\n        self.yield_rate = yield_rate\n        self.capacity_factor = capacity_factor\n        self.tariff = tariff\n        self.opex_per_ton = opex_per_ton\n        self.fixed_opex = fixed_opex\n        self.capex = capex\n        self.discount_rate = discount_rate\n        self.carbon_credit_price = carbon_credit_price\n        self.byproduct_price = byproduct_price\n        self.enable_byproduct = enable_byproduct\n    \n    def calculate_annual_metrics(self, daily_waste_tons: float, year: int, \n                               growth_rate: float = 0.02) -> Dict[str, float]:\n        \"\"\"Calculate annual metrics for a given year\"\"\"\n        \n        # Calculate waste for this year with growth\n        annual_waste_tons = daily_waste_tons * 365 * ((1 + growth_rate) ** (year - 1))\n        \n        # Energy generation\n        annual_kwh = annual_waste_tons * self.yield_rate * self.capacity_factor\n        \n        # Revenue from electricity\n        electricity_revenue = annual_kwh * self.tariff\n        \n        # Carbon credits revenue (1 credit per ton CO2 saved)\n        co2_saved_tons = (annual_kwh * CO2_PER_KWH_KG) / 1000\n        carbon_revenue = co2_saved_tons * self.carbon_credit_price\n        \n        # Byproduct revenue (conservative: 0.3 tons byproduct per ton waste)\n        byproduct_revenue = 0.0\n        if self.enable_byproduct:\n            byproduct_tons = annual_waste_tons * 0.3  # Conservative ratio\n            byproduct_revenue = byproduct_tons * self.byproduct_price\n        \n        # Total revenue\n        annual_revenue = electricity_revenue + carbon_revenue + byproduct_revenue\n        \n        # Operating expenses\n        variable_opex = annual_waste_tons * self.opex_per_ton\n        total_opex = variable_opex + self.fixed_opex\n        \n        # Net cash flow\n        ncf = annual_revenue - total_opex\n        \n        return {\n            'year': year,\n            'waste_tons': annual_waste_tons,\n            'electricity_kwh': annual_kwh,\n            'electricity_revenue': electricity_revenue,\n            'carbon_revenue': carbon_revenue,\n            'byproduct_revenue': byproduct_revenue,\n            'revenue': annual_revenue,\n            'variable_opex': variable_opex,\n            'fixed_opex': self.fixed_opex,\n            'total_opex': total_opex,\n            'ncf': ncf,\n            'co2_saved_tons': co2_saved_tons\n        }\n    \n    def calculate_cashflows(self, daily_waste_tons: float, horizon_years: int,\n                          growth_rate: float = 0.02) -> List[Dict[str, float]]:\n        \"\"\"Calculate cashflows for project horizon\"\"\"\n        \n        cashflows = []\n        \n        for year in range(1, horizon_years + 1):\n            annual_metrics = self.calculate_annual_metrics(\n                daily_waste_tons, year, growth_rate\n            )\n            \n            cashflow_item = {\n                'year': year,\n                'waste_tons': annual_metrics['waste_tons'],\n                'electricity_kwh': annual_metrics['electricity_kwh'],\n                'revenue': annual_metrics['revenue'],\n                'opex': annual_metrics['total_opex'],\n                'ncf': annual_metrics['ncf']\n            }\n            \n            cashflows.append(cashflow_item)\n        \n        return cashflows\n    \n    def calculate_npv(self, daily_waste_tons: float, horizon_years: int,\n                      growth_rate: float = 0.02) -> float:\n        \"\"\"Calculate Net Present Value\"\"\"\n        \n        cashflows = self.calculate_cashflows(daily_waste_tons, horizon_years, growth_rate)\n        \n        # Calculate discounted cash flows\n        npv = -self.capex  # Initial investment\n        \n        for cf in cashflows:\n            year = cf['year']\n            discounted_ncf = cf['ncf'] / ((1 + self.discount_rate) ** year)\n            npv += discounted_ncf\n        \n        return npv\n    \n    def calculate_payback(self, daily_waste_tons: float, horizon_years: int,\n                         growth_rate: float = 0.02) -> float:\n        \"\"\"Calculate payback period in years\"\"\"\n        \n        cashflows = self.calculate_cashflows(daily_waste_tons, horizon_years, growth_rate)\n        \n        cumulative_ncf = 0\n        \n        for cf in cashflows:\n            cumulative_ncf += cf['ncf']\n            \n            if cumulative_ncf >= self.capex:\n                # Linear interpolation for more precise payback\n                prev_cumulative = cumulative_ncf - cf['ncf']\n                remaining_recovery = self.capex - prev_cumulative\n                year_fraction = remaining_recovery / cf['ncf']\n                \n                return cf['year'] - 1 + year_fraction\n        \n        return float('inf')  # Payback not achieved within horizon\n    \n    def calculate_roi(self, daily_waste_tons: float, horizon_years: int,\n                      growth_rate: float = 0.02) -> float:\n        \"\"\"Calculate Return on Investment percentage\"\"\"\n        \n        cashflows = self.calculate_cashflows(daily_waste_tons, horizon_years, growth_rate)\n        \n        total_ncf = sum(cf['ncf'] for cf in cashflows)\n        \n        if self.capex > 0:\n            roi = (total_ncf / self.capex) * 100\n        else:\n            roi = 0\n        \n        return roi\n    \n    def calculate_irr(self, daily_waste_tons: float, horizon_years: int,\n                      growth_rate: float = 0.02) -> float:\n        \"\"\"Calculate Internal Rate of Return using approximation\"\"\"\n        \n        cashflows = self.calculate_cashflows(daily_waste_tons, horizon_years, growth_rate)\n        \n        # Simple IRR approximation\n        total_ncf = sum(cf['ncf'] for cf in cashflows)\n        avg_annual_ncf = total_ncf / horizon_years\n        \n        if self.capex > 0:\n            irr_approx = (avg_annual_ncf / self.capex) * 100\n        else:\n            irr_approx = 0\n        \n        return max(0, irr_approx)\n    \n    def calculate_environmental_impact(self, daily_waste_tons: float, \n                                     horizon_years: int,\n                                     growth_rate: float = 0.02) -> Dict[str, float]:\n        \"\"\"Calculate environmental impact metrics\"\"\"\n        \n        cashflows = self.calculate_cashflows(daily_waste_tons, horizon_years, growth_rate)\n        \n        # Total electricity generation\n        total_kwh = sum(cf['electricity_kwh'] for cf in cashflows)\n        \n        # CO2 savings (kg CO2/kWh)\n        co2_savings_kg = total_kwh * CO2_PER_KWH_KG\n        co2_savings_tons = co2_savings_kg / 1000\n        \n        # Tree equivalent (1 tree = 20 kg CO2/year, project lifetime average)\n        trees_equivalent = int(co2_savings_tons * 50)  # Conservative estimate\n        \n        return {\n            'total_electricity_kwh': total_kwh,\n            'co2_savings_kg': co2_savings_kg,\n            'co2_savings_tons': co2_savings_tons,\n            'trees_equivalent': trees_equivalent\n        }\n    \n    def sensitivity_analysis(self, daily_waste_tons: float, horizon_years: int,\n                           parameter_variations: Dict[str, List[float]]) -> Dict[str, List[float]]:\n        \"\"\"Perform sensitivity analysis on key parameters\"\"\"\n        \n        results = {}\n        base_npv = self.calculate_npv(daily_waste_tons, horizon_years)\n        \n        for param_name, variations in parameter_variations.items():\n            npv_variations = []\n            \n            for variation in variations:\n                # Create modified calculator\n                modified_calc = self._create_modified_calculator(param_name, variation)\n                modified_npv = modified_calc.calculate_npv(daily_waste_tons, horizon_years)\n                npv_variations.append(modified_npv)\n            \n            results[param_name] = npv_variations\n        \n        return results\n    \n    def _create_modified_calculator(self, param_name: str, value: float) -> 'FinanceCalculator':\n        \"\"\"Create calculator with modified parameter\"\"\"\n        \n        params = {\n            'yield_rate': self.yield_rate,\n            'capacity_factor': self.capacity_factor,\n            'tariff': self.tariff,\n            'opex_per_ton': self.opex_per_ton,\n            'fixed_opex': self.fixed_opex,\n            'capex': self.capex,\n            'discount_rate': self.discount_rate,\n            'carbon_credit_price': self.carbon_credit_price,\n            'byproduct_price': self.byproduct_price,\n            'enable_byproduct': self.enable_byproduct\n        }\n        \n        if param_name in params:\n            params[param_name] = value\n        \n        return FinanceCalculator(**params)\n    \n    def generate_financial_summary(self, daily_waste_tons: float, horizon_years: int,\n                                 growth_rate: float = 0.02) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive financial summary\"\"\"\n        \n        # Core metrics\n        npv = self.calculate_npv(daily_waste_tons, horizon_years, growth_rate)\n        payback = self.calculate_payback(daily_waste_tons, horizon_years, growth_rate)\n        roi = self.calculate_roi(daily_waste_tons, horizon_years, growth_rate)\n        irr = self.calculate_irr(daily_waste_tons, horizon_years, growth_rate)\n        \n        # Environmental impact\n        env_impact = self.calculate_environmental_impact(daily_waste_tons, horizon_years, growth_rate)\n        \n        # Cashflow summary\n        cashflows = self.calculate_cashflows(daily_waste_tons, horizon_years, growth_rate)\n        total_revenue = sum(cf['revenue'] for cf in cashflows)\n        total_opex = sum(cf['opex'] for cf in cashflows)\n        \n        return {\n            'financial_metrics': {\n                'npv': npv,\n                'payback_years': payback,\n                'roi_percent': roi,\n                'irr_percent': irr,\n                'total_revenue': total_revenue,\n                'total_opex': total_opex,\n                'total_ncf': total_revenue - total_opex\n            },\n            'environmental_impact': env_impact,\n            'project_parameters': {\n                'daily_waste_tons': daily_waste_tons,\n                'horizon_years': horizon_years,\n                'growth_rate': growth_rate,\n                'capex': self.capex,\n                'yield_rate': self.yield_rate,\n                'capacity_factor': self.capacity_factor,\n                'tariff': self.tariff\n            }\n        }\n    \n    def get_key_assumptions(self) -> Dict[str, Any]:\n        \"\"\"Get key financial assumptions\"\"\"\n        \n        return {\n            'technical_assumptions': {\n                'yield_rate_kwh_per_ton': self.yield_rate,\n                'capacity_factor_percent': self.capacity_factor * 100,\n                'plant_availability': '24/7 operation assumed'\n            },\n            'financial_assumptions': {\n                'tariff_inr_per_kwh': self.tariff,\n                'discount_rate_percent': self.discount_rate * 100,\n                'inflation_rate': 'Not explicitly modeled',\n                'tax_considerations': 'Not included in current model'\n            },\n            'operating_assumptions': {\n                'opex_per_ton_inr': self.opex_per_ton,\n                'fixed_opex_inr_per_year': self.fixed_opex,\n                'escalation_rates': 'Constant real terms assumed'\n            },\n            'environmental_assumptions': {\n                'co2_emission_factor': f'{CO2_PER_KWH_KG} kg CO2/kWh',\n                'grid_displacement': 'Full grid electricity displacement assumed',\n                'tree_equivalency': '1 tree = 20 kg CO2/year sequestration'\n            }\n        }\n","size_bytes":12844},"bia_core/maps.py":{"content":"\"\"\"\nMapping functionality for bioenergy facilities.\n\"\"\"\n\nimport folium\nimport pandas as pd\nfrom typing import Dict, Any, Optional, List\nimport streamlit as st\n\n# Default map coordinates for supported cities\nCITY_COORDINATES = {\n    'Ahmedabad': [23.0225, 72.5714],\n    'Gandhinagar': [23.2156, 72.6369],\n    'Indore': [22.7196, 75.8577],\n    'Delhi': [28.7041, 77.1025],\n    'Mumbai': [19.0760, 72.8777],\n    'Pune': [18.5204, 73.8567],\n    'Bengaluru': [12.9716, 77.5946],\n    'Chennai': [13.0827, 80.2707]\n}\n\ndef create_facilities_map(facilities_df: pd.DataFrame, city: str, \n                         zoom_start: int = 10) -> folium.Map:\n    \"\"\"\n    Create interactive map with bioenergy facilities\n    \n    Args:\n        facilities_df: DataFrame with facility data\n        city: City name for map centering\n        zoom_start: Initial zoom level\n    \n    Returns:\n        Folium map object\n    \"\"\"\n    \n    # Get city coordinates\n    if city in CITY_COORDINATES:\n        center_coords = CITY_COORDINATES[city]\n    else:\n        # Default to India center if city not found\n        center_coords = [20.5937, 78.9629]\n    \n    # Create base map\n    m = folium.Map(\n        location=center_coords,\n        zoom_start=zoom_start,\n        tiles='OpenStreetMap'\n    )\n    \n    # Add facilities as markers\n    if not facilities_df.empty:\n        for idx, facility in facilities_df.iterrows():\n            try:\n                # Determine marker color based on status\n                status = facility.get('status', 'unknown').lower()\n                if status == 'operational':\n                    color = 'green'\n                    icon = 'play'\n                elif status == 'under_construction':\n                    color = 'orange'\n                    icon = 'cog'\n                elif status == 'planned':\n                    color = 'blue'\n                    icon = 'clock'\n                else:\n                    color = 'gray'\n                    icon = 'question'\n                \n                # Create popup content\n                popup_content = create_facility_popup(facility)\n                \n                # Add marker\n                folium.Marker(\n                    location=[facility['lat'], facility['lon']],\n                    popup=folium.Popup(popup_content, max_width=300),\n                    tooltip=facility['name'],\n                    icon=folium.Icon(\n                        color=color,\n                        icon=icon,\n                        prefix='fa'\n                    )\n                ).add_to(m)\n                \n            except Exception as e:\n                print(f\"Error adding facility marker: {e}\")\n                continue\n    \n    # Add city marker\n    folium.Marker(\n        location=center_coords,\n        popup=f\"{city} City Center\",\n        tooltip=f\"{city}\",\n        icon=folium.Icon(\n            color='red',\n            icon='home',\n            prefix='fa'\n        )\n    ).add_to(m)\n    \n    # Add legend\n    add_map_legend(m)\n    \n    return m\n\ndef create_facility_popup(facility: pd.Series) -> str:\n    \"\"\"Create HTML popup content for facility marker\"\"\"\n    \n    # Format capacity\n    capacity = facility.get('capacity_mw', 0)\n    if capacity > 0:\n        capacity_str = f\"{capacity:.1f} MW\"\n    else:\n        capacity_str = \"Not specified\"\n    \n    # Create popup HTML\n    popup_html = f\"\"\"\n    <div style=\"width: 250px;\">\n        <h4 style=\"margin: 0 0 10px 0; color: #2E8B57;\">\n            {facility.get('name', 'Unknown Facility')}\n        </h4>\n        \n        <table style=\"width: 100%; font-size: 12px;\">\n            <tr>\n                <td><strong>Type:</strong></td>\n                <td>{facility.get('type', 'Unknown')}</td>\n            </tr>\n            <tr>\n                <td><strong>Capacity:</strong></td>\n                <td>{capacity_str}</td>\n            </tr>\n            <tr>\n                <td><strong>Status:</strong></td>\n                <td style=\"color: {get_status_color(facility.get('status', 'unknown'))};\">\n                    {format_status(facility.get('status', 'unknown'))}\n                </td>\n            </tr>\n            <tr>\n                <td><strong>Location:</strong></td>\n                <td>{facility.get('city', '')}, {facility.get('state', '')}</td>\n            </tr>\n            <tr>\n                <td><strong>Source:</strong></td>\n                <td style=\"font-size: 10px;\">{facility.get('source', 'Unknown')}</td>\n            </tr>\n        </table>\n        \n        <div style=\"margin-top: 10px; font-size: 10px; color: #666;\">\n            Coordinates: {facility.get('lat', 0):.4f}, {facility.get('lon', 0):.4f}\n        </div>\n    </div>\n    \"\"\"\n    \n    return popup_html\n\ndef get_status_color(status: str) -> str:\n    \"\"\"Get color for facility status\"\"\"\n    status_colors = {\n        'operational': '#28a745',\n        'under_construction': '#fd7e14',\n        'planned': '#007bff',\n        'unknown': '#6c757d'\n    }\n    \n    return status_colors.get(status.lower(), '#6c757d')\n\ndef format_status(status: str) -> str:\n    \"\"\"Format status string for display\"\"\"\n    if status.lower() == 'under_construction':\n        return 'Under Construction'\n    elif status.lower() == 'operational':\n        return 'Operational'\n    elif status.lower() == 'planned':\n        return 'Planned'\n    else:\n        return 'Unknown'\n\ndef add_map_legend(map_obj: folium.Map):\n    \"\"\"Add legend to the map\"\"\"\n    \n    legend_html = '''\n    <div style=\"position: fixed; \n                bottom: 50px; left: 50px; width: 180px; height: 120px; \n                background-color: white; border:2px solid grey; z-index:9999; \n                font-size:14px; padding: 10px\">\n    <h5 style=\"margin: 0 0 10px 0;\">Facility Status</h5>\n    <p style=\"margin: 5px 0;\"><i class=\"fa fa-circle\" style=\"color:green\"></i> Operational</p>\n    <p style=\"margin: 5px 0;\"><i class=\"fa fa-circle\" style=\"color:orange\"></i> Under Construction</p>\n    <p style=\"margin: 5px 0;\"><i class=\"fa fa-circle\" style=\"color:blue\"></i> Planned</p>\n    <p style=\"margin: 5px 0;\"><i class=\"fa fa-circle\" style=\"color:red\"></i> City Center</p>\n    </div>\n    '''\n    \n    map_obj.get_root().html.add_child(folium.Element(legend_html))\n\ndef create_heat_map(facilities_df: pd.DataFrame, city: str) -> folium.Map:\n    \"\"\"Create heat map of facility density\"\"\"\n    \n    from folium.plugins import HeatMap\n    \n    # Get city coordinates\n    center_coords = CITY_COORDINATES.get(city, [20.5937, 78.9629])\n    \n    # Create base map\n    m = folium.Map(\n        location=center_coords,\n        zoom_start=10,\n        tiles='OpenStreetMap'\n    )\n    \n    if not facilities_df.empty:\n        # Prepare heat map data\n        heat_data = []\n        for idx, facility in facilities_df.iterrows():\n            try:\n                # Weight by capacity if available\n                weight = facility.get('capacity_mw', 1)\n                heat_data.append([\n                    facility['lat'], \n                    facility['lon'], \n                    weight\n                ])\n            except:\n                continue\n        \n        # Add heat map layer\n        if heat_data:\n            HeatMap(heat_data).add_to(m)\n    \n    return m\n\ndef get_facility_statistics(facilities_df: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"Get statistics about facilities\"\"\"\n    \n    if facilities_df.empty:\n        return {\n            'total_facilities': 0,\n            'total_capacity_mw': 0,\n            'operational_count': 0,\n            'planned_count': 0,\n            'under_construction_count': 0,\n            'avg_capacity_mw': 0,\n            'facility_types': {}\n        }\n    \n    # Basic statistics\n    total_facilities = len(facilities_df)\n    total_capacity = facilities_df['capacity_mw'].sum()\n    avg_capacity = facilities_df['capacity_mw'].mean()\n    \n    # Status counts\n    status_counts = facilities_df['status'].value_counts().to_dict()\n    operational_count = status_counts.get('operational', 0)\n    planned_count = status_counts.get('planned', 0)\n    under_construction_count = status_counts.get('under_construction', 0)\n    \n    # Facility types\n    type_counts = facilities_df['type'].value_counts().to_dict()\n    \n    return {\n        'total_facilities': total_facilities,\n        'total_capacity_mw': total_capacity,\n        'operational_count': operational_count,\n        'planned_count': planned_count,\n        'under_construction_count': under_construction_count,\n        'avg_capacity_mw': avg_capacity,\n        'facility_types': type_counts\n    }\n\ndef filter_facilities_by_criteria(facilities_df: pd.DataFrame, \n                                criteria: Dict[str, Any]) -> pd.DataFrame:\n    \"\"\"Filter facilities based on criteria\"\"\"\n    \n    filtered_df = facilities_df.copy()\n    \n    # Filter by status\n    if 'status' in criteria and criteria['status']:\n        filtered_df = filtered_df[filtered_df['status'].isin(criteria['status'])]\n    \n    # Filter by type\n    if 'type' in criteria and criteria['type']:\n        filtered_df = filtered_df[filtered_df['type'].isin(criteria['type'])]\n    \n    # Filter by capacity range\n    if 'min_capacity' in criteria:\n        filtered_df = filtered_df[filtered_df['capacity_mw'] >= criteria['min_capacity']]\n    \n    if 'max_capacity' in criteria:\n        filtered_df = filtered_df[filtered_df['capacity_mw'] <= criteria['max_capacity']]\n    \n    return filtered_df\n\n@st.cache_data\ndef load_facility_data_for_city(city: str) -> pd.DataFrame:\n    \"\"\"Load facility data for specific city with caching\"\"\"\n    \n    from bia_core.data_io import load_curated_data\n    \n    try:\n        curated_data = load_curated_data()\n        facilities = curated_data.get('facilities', pd.DataFrame())\n        \n        if not facilities.empty:\n            city_facilities = facilities[facilities['city'] == city].copy()\n            return city_facilities\n        else:\n            return pd.DataFrame()\n            \n    except Exception as e:\n        print(f\"Error loading facility data for {city}: {e}\")\n        return pd.DataFrame()\n","size_bytes":10038},"bia_core/models.py":{"content":"\"\"\"\nForecasting models for waste prediction.\nIncludes deterministic and SARIMA models with model selection.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Optional, Dict, Any\nimport warnings\nfrom datetime import datetime, timedelta\n\n# Try to import statsmodels, fallback if not available\ntry:\n    from statsmodels.tsa.seasonal import seasonal_decompose\n    from statsmodels.tsa.statespace.sarimax import SARIMAX\n    STATSMODELS_AVAILABLE = True\nexcept ImportError:\n    STATSMODELS_AVAILABLE = False\n\nclass BaseModel:\n    \"\"\"Base class for forecasting models\"\"\"\n    \n    def __init__(self):\n        self.is_fitted = False\n        self.model_params = {}\n        self.last_mape = float('inf')\n    \n    def fit(self, features_df: pd.DataFrame):\n        \"\"\"Fit the model to historical data\"\"\"\n        raise NotImplementedError\n    \n    def predict(self, forecast_days: int) -> List[float]:\n        \"\"\"Generate forecast for specified number of days\"\"\"\n        raise NotImplementedError\n    \n    def get_model_info(self) -> Dict[str, Any]:\n        \"\"\"Get model information\"\"\"\n        return {\n            'name': self.__class__.__name__,\n            'is_fitted': self.is_fitted,\n            'parameters': self.model_params,\n            'last_mape': self.last_mape\n        }\n\nclass DeterministicModel(BaseModel):\n    \"\"\"Simple deterministic growth model\"\"\"\n    \n    def __init__(self, default_growth_rate: float = 0.002):\n        super().__init__()\n        self.default_growth_rate = default_growth_rate\n        self.base_value = 0\n        self.growth_rate = default_growth_rate\n    \n    def fit(self, features_df: pd.DataFrame):\n        \"\"\"Fit deterministic model\"\"\"\n        \n        if features_df.empty or 'waste_tons' not in features_df.columns:\n            self.base_value = 1.0\n            self.growth_rate = self.default_growth_rate\n            self.is_fitted = True\n            return\n        \n        # Calculate base value (recent average)\n        recent_data = features_df['waste_tons'].tail(7)  # Last 7 days\n        self.base_value = recent_data.mean() if len(recent_data) > 0 else 1.0\n        \n        # Calculate growth rate\n        if len(features_df) >= 14:\n            # Compare first and second half\n            mid_point = len(features_df) // 2\n            first_half_avg = features_df['waste_tons'][:mid_point].mean()\n            second_half_avg = features_df['waste_tons'][mid_point:].mean()\n            \n            if first_half_avg > 0:\n                total_periods = len(features_df) - mid_point\n                total_growth = (second_half_avg / first_half_avg) - 1\n                self.growth_rate = (1 + total_growth) ** (1/total_periods) - 1\n                \n                # Cap growth rate\n                self.growth_rate = max(-0.01, min(0.01, self.growth_rate))\n            else:\n                self.growth_rate = self.default_growth_rate\n        else:\n            self.growth_rate = self.default_growth_rate\n        \n        self.model_params = {\n            'base_value': self.base_value,\n            'growth_rate': self.growth_rate\n        }\n        \n        self.is_fitted = True\n    \n    def predict(self, forecast_days: int) -> List[float]:\n        \"\"\"Generate deterministic forecast\"\"\"\n        \n        if not self.is_fitted:\n            return [1.0] * forecast_days\n        \n        forecast = []\n        for t in range(1, forecast_days + 1):\n            value = self.base_value * ((1 + self.growth_rate) ** t)\n            forecast.append(max(0, value))  # Ensure non-negative\n        \n        return forecast\n\nclass SARIMAModel(BaseModel):\n    \"\"\"SARIMA time series model\"\"\"\n    \n    def __init__(self, order=(1,1,1), seasonal_order=(0,1,1,12)):\n        super().__init__()\n        self.order = order\n        self.seasonal_order = seasonal_order\n        self.model = None\n        self.fitted_model = None\n        self.base_forecast = None\n    \n    def fit(self, features_df: pd.DataFrame):\n        \"\"\"Fit SARIMA model\"\"\"\n        \n        if not STATSMODELS_AVAILABLE:\n            # Fallback to deterministic if statsmodels not available\n            self.base_forecast = DeterministicModel()\n            self.base_forecast.fit(features_df)\n            self.is_fitted = True\n            return\n        \n        if features_df.empty or len(features_df) < 10:\n            # Need sufficient data for SARIMA\n            self.base_forecast = DeterministicModel()\n            self.base_forecast.fit(features_df)\n            self.is_fitted = True\n            return\n        \n        try:\n            # Prepare time series data\n            ts_data = features_df.set_index('date')['waste_tons']\n            ts_data = ts_data.asfreq('D', fill_value=0)\n            \n            # Handle zero variance\n            if ts_data.std() == 0:\n                self.base_forecast = DeterministicModel()\n                self.base_forecast.fit(features_df)\n                self.is_fitted = True\n                return\n            \n            # Fit SARIMA model\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                \n                self.model = SARIMAX(\n                    ts_data,\n                    order=self.order,\n                    seasonal_order=self.seasonal_order,\n                    enforce_stationarity=False,\n                    enforce_invertibility=False\n                )\n                \n                self.fitted_model = self.model.fit(disp=False)\n                \n                self.model_params = {\n                    'order': self.order,\n                    'seasonal_order': self.seasonal_order,\n                    'aic': self.fitted_model.aic,\n                    'bic': self.fitted_model.bic\n                }\n                \n                self.is_fitted = True\n                \n        except Exception as e:\n            # Fallback to deterministic model\n            print(f\"SARIMA fitting failed: {e}\")\n            self.base_forecast = DeterministicModel()\n            self.base_forecast.fit(features_df)\n            self.is_fitted = True\n    \n    def predict(self, forecast_days: int) -> List[float]:\n        \"\"\"Generate SARIMA forecast\"\"\"\n        \n        if not self.is_fitted:\n            return [1.0] * forecast_days\n        \n        # Use fallback model if SARIMA failed\n        if self.base_forecast is not None:\n            return self.base_forecast.predict(forecast_days)\n        \n        if self.fitted_model is None:\n            return [1.0] * forecast_days\n        \n        try:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                \n                forecast_result = self.fitted_model.forecast(steps=forecast_days)\n                forecast = forecast_result.tolist()\n                \n                # Ensure non-negative values\n                forecast = [max(0, value) for value in forecast]\n                \n                return forecast\n                \n        except Exception as e:\n            print(f\"SARIMA prediction failed: {e}\")\n            # Return simple linear forecast\n            return [1.0] * forecast_days\n\nclass ModelSelector:\n    \"\"\"Select best model based on performance\"\"\"\n    \n    def __init__(self, models: List[BaseModel]):\n        self.models = models\n        self.best_model = None\n        self.performance_scores = {}\n    \n    def select_best_model(self, features_df: pd.DataFrame) -> BaseModel:\n        \"\"\"Select best performing model\"\"\"\n        \n        if len(self.models) == 0:\n            return DeterministicModel()\n        \n        if len(features_df) < 10:\n            # Insufficient data for proper evaluation\n            return self.models[0]\n        \n        from bia_core.eval import backtest_model\n        \n        best_score = float('inf')\n        best_model = self.models[0]\n        \n        for model in self.models:\n            try:\n                # Fit model\n                model.fit(features_df)\n                \n                # Calculate backtest score\n                score = backtest_model(model, features_df)\n                self.performance_scores[type(model).__name__] = score\n                \n                if score < best_score:\n                    best_score = score\n                    best_model = model\n                    \n            except Exception as e:\n                print(f\"Model evaluation failed for {type(model).__name__}: {e}\")\n                self.performance_scores[type(model).__name__] = float('inf')\n        \n        self.best_model = best_model\n        return best_model\n    \n    def get_performance_summary(self) -> Dict[str, float]:\n        \"\"\"Get performance summary for all models\"\"\"\n        return self.performance_scores.copy()\n\ndef create_ensemble_forecast(models: List[BaseModel], features_df: pd.DataFrame, \n                           forecast_days: int, weights: Optional[List[float]] = None) -> List[float]:\n    \"\"\"Create ensemble forecast from multiple models\"\"\"\n    \n    if not models:\n        return [1.0] * forecast_days\n    \n    if weights is None:\n        weights = [1.0 / len(models)] * len(models)\n    \n    ensemble_forecast = [0.0] * forecast_days\n    \n    for model, weight in zip(models, weights):\n        try:\n            model.fit(features_df)\n            model_forecast = model.predict(forecast_days)\n            \n            for i in range(forecast_days):\n                ensemble_forecast[i] += weight * model_forecast[i]\n                \n        except Exception as e:\n            print(f\"Ensemble model failed: {type(model).__name__}: {e}\")\n    \n    return ensemble_forecast\n","size_bytes":9587},"bia_core/schemas.py":{"content":"\"\"\"\nData schemas and models for BIA application using Pydantic.\n\"\"\"\n\nfrom pydantic import BaseModel, Field, validator\nfrom typing import Optional, List\nfrom datetime import date, datetime\nfrom bia_core import SUPPORTED_CITIES, WASTE_TYPES\n\nclass UserProfile(BaseModel):\n    \"\"\"User profile data model\"\"\"\n    username: str = Field(..., min_length=3, max_length=50)\n    password_hash: str\n    entity_name: str = Field(..., min_length=2, max_length=100)\n    city: str\n    waste_type: str\n    created_at: Optional[datetime] = Field(default_factory=datetime.now)\n    \n    @validator('city')\n    def validate_city(cls, v):\n        if v not in SUPPORTED_CITIES:\n            raise ValueError(f\"City must be one of: {', '.join(SUPPORTED_CITIES)}\")\n        return v\n    \n    @validator('waste_type')\n    def validate_waste_type(cls, v):\n        if v not in WASTE_TYPES:\n            raise ValueError(f\"Waste type must be one of: {', '.join(WASTE_TYPES)}\")\n        return v\n\nclass WasteLog(BaseModel):\n    \"\"\"Waste log entry data model\"\"\"\n    username: str\n    date: date\n    waste_tons: float = Field(..., gt=0, le=1000)\n    created_at: Optional[datetime] = Field(default_factory=datetime.now)\n    \n    @validator('waste_tons')\n    def validate_waste_amount(cls, v):\n        if v <= 0:\n            raise ValueError(\"Waste amount must be positive\")\n        if v > 1000:\n            raise ValueError(\"Waste amount seems unreasonably high (>1000 tons)\")\n        return v\n\nclass ForecastInput(BaseModel):\n    \"\"\"Input parameters for forecasting\"\"\"\n    historical_data: List[float]\n    forecast_horizon: int = Field(..., ge=1, le=365)\n    growth_rate: Optional[float] = Field(default=0.02, ge=-0.5, le=1.0)\n    \n    @validator('forecast_horizon')\n    def validate_horizon(cls, v):\n        if v < 1 or v > 365:\n            raise ValueError(\"Forecast horizon must be between 1 and 365 days\")\n        return v\n\nclass FinancialParameters(BaseModel):\n    \"\"\"Financial calculation parameters\"\"\"\n    yield_rate: float = Field(..., gt=0, le=3000)  # kWh/ton\n    capacity_factor: float = Field(..., gt=0, le=1)  # fraction\n    tariff: float = Field(..., gt=0, le=20)  # ₹/kWh\n    opex_per_ton: float = Field(..., ge=0, le=5000)  # ₹/ton\n    fixed_opex: float = Field(..., ge=0)  # ₹/year\n    capex: float = Field(..., gt=0)  # ₹\n    discount_rate: float = Field(..., gt=0, le=1)  # fraction\n    horizon_years: int = Field(..., ge=1, le=50)\n    \n    @validator('yield_rate')\n    def validate_yield(cls, v):\n        if v < 100 or v > 3000:\n            raise ValueError(\"Yield rate should be between 100-3000 kWh/ton\")\n        return v\n    \n    @validator('capacity_factor')\n    def validate_capacity_factor(cls, v):\n        if v <= 0 or v > 1:\n            raise ValueError(\"Capacity factor must be between 0 and 1\")\n        return v\n    \n    @validator('tariff')\n    def validate_tariff(cls, v):\n        if v <= 0 or v > 20:\n            raise ValueError(\"Tariff should be between 0-20 ₹/kWh\")\n        return v\n\nclass FacilityData(BaseModel):\n    \"\"\"Facility information data model\"\"\"\n    name: str\n    city: str\n    state: str\n    type: str\n    capacity_mw: float = Field(..., ge=0)\n    status: str\n    lat: float = Field(..., ge=-90, le=90)\n    lon: float = Field(..., ge=-180, le=180)\n    source: str\n    \n    @validator('capacity_mw')\n    def validate_capacity(cls, v):\n        if v < 0:\n            raise ValueError(\"Capacity cannot be negative\")\n        return v\n\nclass CashflowItem(BaseModel):\n    \"\"\"Single year cashflow item\"\"\"\n    year: int\n    waste_tons: float\n    electricity_kwh: float\n    revenue: float\n    opex: float\n    ncf: float  # Net cash flow\n    \n    @validator('year')\n    def validate_year(cls, v):\n        if v < 1:\n            raise ValueError(\"Year must be positive\")\n        return v\n\nclass NPVResults(BaseModel):\n    \"\"\"NPV calculation results\"\"\"\n    npv: float\n    payback_years: float\n    roi_percent: float\n    total_revenue: float\n    total_opex: float\n    co2_savings_tons: float\n    trees_equivalent: int\n","size_bytes":4025},"bia_core/utils.py":{"content":"\"\"\"\nUtility functions for BIA application.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom typing import Any, Dict, List, Optional, Union\nimport re\nfrom datetime import datetime, date\nimport json\n\ndef format_currency(amount: float, currency: str = \"₹\") -> str:\n    \"\"\"Format currency with Indian number system\"\"\"\n    \n    if pd.isna(amount) or amount == 0:\n        return f\"{currency}0\"\n    \n    # Convert to absolute value for formatting\n    abs_amount = abs(amount)\n    sign = \"-\" if amount < 0 else \"\"\n    \n    # Format based on magnitude\n    if abs_amount >= 1e7:  # Crores\n        formatted = f\"{abs_amount/1e7:.1f} Cr\"\n    elif abs_amount >= 1e5:  # Lakhs\n        formatted = f\"{abs_amount/1e5:.1f} L\"\n    elif abs_amount >= 1e3:  # Thousands\n        formatted = f\"{abs_amount/1e3:.1f}K\"\n    else:\n        formatted = f\"{abs_amount:.0f}\"\n    \n    return f\"{sign}{currency}{formatted}\"\n\ndef format_number(number: float, precision: int = 1) -> str:\n    \"\"\"Format large numbers with K, M, B suffixes\"\"\"\n    \n    if pd.isna(number) or number == 0:\n        return \"0\"\n    \n    abs_number = abs(number)\n    sign = \"-\" if number < 0 else \"\"\n    \n    if abs_number >= 1e9:\n        return f\"{sign}{abs_number/1e9:.{precision}f}B\"\n    elif abs_number >= 1e6:\n        return f\"{sign}{abs_number/1e6:.{precision}f}M\"\n    elif abs_number >= 1e3:\n        return f\"{sign}{abs_number/1e3:.{precision}f}K\"\n    else:\n        return f\"{sign}{abs_number:.{precision}f}\"\n\ndef validate_range(value: float, min_val: float, max_val: float, \n                  param_name: str) -> bool:\n    \"\"\"Validate if value is within acceptable range\"\"\"\n    \n    if pd.isna(value):\n        return False\n    \n    if value < min_val or value > max_val:\n        return False\n    \n    return True\n\ndef clean_string(text: str) -> str:\n    \"\"\"Clean and normalize string input\"\"\"\n    \n    if not isinstance(text, str):\n        return str(text)\n    \n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text.strip())\n    \n    # Remove special characters (keep alphanumeric, spaces, hyphens, underscores)\n    text = re.sub(r'[^\\w\\s\\-]', '', text)\n    \n    return text\n\ndef safe_divide(numerator: float, denominator: float, \n               default: float = 0.0) -> float:\n    \"\"\"Safely divide two numbers, return default if division by zero\"\"\"\n    \n    if pd.isna(numerator) or pd.isna(denominator):\n        return default\n    \n    if denominator == 0:\n        return default\n    \n    return numerator / denominator\n\ndef calculate_percentage_change(old_value: float, new_value: float) -> float:\n    \"\"\"Calculate percentage change between two values\"\"\"\n    \n    if pd.isna(old_value) or pd.isna(new_value):\n        return 0.0\n    \n    if old_value == 0:\n        return 100.0 if new_value > 0 else 0.0\n    \n    return ((new_value - old_value) / old_value) * 100\n\ndef interpolate_missing_values(series: pd.Series, method: str = 'linear') -> pd.Series:\n    \"\"\"Interpolate missing values in a pandas series\"\"\"\n    \n    if series.empty:\n        return series\n    \n    if method == 'linear':\n        return series.interpolate(method='linear')\n    elif method == 'forward':\n        return series.fillna(method='ffill')\n    elif method == 'backward':\n        return series.fillna(method='bfill')\n    else:\n        return series.fillna(0)\n\ndef create_date_range(start_date: Union[str, date], \n                     end_date: Union[str, date],\n                     freq: str = 'D') -> pd.DatetimeIndex:\n    \"\"\"Create date range between two dates\"\"\"\n    \n    if isinstance(start_date, str):\n        start_date = pd.to_datetime(start_date).date()\n    if isinstance(end_date, str):\n        end_date = pd.to_datetime(end_date).date()\n    \n    return pd.date_range(start=start_date, end=end_date, freq=freq)\n\ndef export_data_to_csv(data: Union[pd.DataFrame, Dict], \n                      filename: str = None) -> str:\n    \"\"\"Export data to CSV format\"\"\"\n    \n    if isinstance(data, dict):\n        df = pd.DataFrame(data)\n    else:\n        df = data.copy()\n    \n    return df.to_csv(index=False)\n\ndef export_data_to_json(data: Any, filename: str = None) -> str:\n    \"\"\"Export data to JSON format\"\"\"\n    \n    if isinstance(data, pd.DataFrame):\n        data_dict = data.to_dict(orient='records')\n    else:\n        data_dict = data\n    \n    return json.dumps(data_dict, indent=2, default=str)\n\ndef validate_email(email: str) -> bool:\n    \"\"\"Validate email format\"\"\"\n    \n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return re.match(pattern, email) is not None\n\ndef validate_phone(phone: str) -> bool:\n    \"\"\"Validate Indian phone number format\"\"\"\n    \n    # Remove spaces and special characters\n    clean_phone = re.sub(r'[^\\d]', '', phone)\n    \n    # Check if it's 10 digits or 10 digits with country code\n    if len(clean_phone) == 10:\n        return clean_phone.startswith(('6', '7', '8', '9'))\n    elif len(clean_phone) == 12:\n        return clean_phone.startswith('91') and clean_phone[2:].startswith(('6', '7', '8', '9'))\n    \n    return False\n\ndef calculate_working_days(start_date: date, end_date: date) -> int:\n    \"\"\"Calculate number of working days between two dates\"\"\"\n    \n    if start_date > end_date:\n        return 0\n    \n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    working_days = len([d for d in date_range if d.weekday() < 5])  # Monday=0, Sunday=6\n    \n    return working_days\n\ndef round_to_significant_figures(number: float, sig_figs: int = 3) -> float:\n    \"\"\"Round number to specified significant figures\"\"\"\n    \n    if number == 0:\n        return 0\n    \n    return round(number, -int(np.floor(np.log10(abs(number)))) + (sig_figs - 1))\n\ndef detect_outliers(series: pd.Series, method: str = 'iqr', \n                   threshold: float = 1.5) -> pd.Series:\n    \"\"\"Detect outliers in a pandas series\"\"\"\n    \n    if series.empty:\n        return pd.Series([], dtype=bool)\n    \n    if method == 'iqr':\n        Q1 = series.quantile(0.25)\n        Q3 = series.quantile(0.75)\n        IQR = Q3 - Q1\n        \n        lower_bound = Q1 - threshold * IQR\n        upper_bound = Q3 + threshold * IQR\n        \n        outliers = (series < lower_bound) | (series > upper_bound)\n        \n    elif method == 'zscore':\n        z_scores = np.abs((series - series.mean()) / series.std())\n        outliers = z_scores > threshold\n        \n    else:\n        outliers = pd.Series([False] * len(series), index=series.index)\n    \n    return outliers\n\ndef create_summary_statistics(df: pd.DataFrame, \n                            columns: List[str] = None) -> pd.DataFrame:\n    \"\"\"Create summary statistics for dataframe columns\"\"\"\n    \n    if columns is None:\n        columns = df.select_dtypes(include=[np.number]).columns.tolist()\n    \n    summary_stats = []\n    \n    for col in columns:\n        if col in df.columns:\n            stats = {\n                'Column': col,\n                'Count': df[col].count(),\n                'Mean': df[col].mean(),\n                'Std': df[col].std(),\n                'Min': df[col].min(),\n                '25%': df[col].quantile(0.25),\n                '50%': df[col].median(),\n                '75%': df[col].quantile(0.75),\n                'Max': df[col].max(),\n                'Missing': df[col].isna().sum(),\n                'Outliers': detect_outliers(df[col]).sum()\n            }\n            summary_stats.append(stats)\n    \n    return pd.DataFrame(summary_stats)\n\ndef generate_color_palette(n_colors: int, palette: str = 'viridis') -> List[str]:\n    \"\"\"Generate color palette for visualizations\"\"\"\n    \n    import matplotlib.pyplot as plt\n    \n    if palette in plt.colormaps():\n        cmap = plt.get_cmap(palette)\n        colors = [cmap(i / (n_colors - 1)) for i in range(n_colors)]\n        # Convert to hex\n        hex_colors = ['#%02x%02x%02x' % (int(r*255), int(g*255), int(b*255)) \n                     for r, g, b, a in colors]\n        return hex_colors\n    else:\n        # Default colors\n        default_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', \n                         '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', \n                         '#bcbd22', '#17becf']\n        return (default_colors * (n_colors // len(default_colors) + 1))[:n_colors]\n\ndef log_performance(func_name: str, execution_time: float, \n                   data_size: int = None) -> Dict[str, Any]:\n    \"\"\"Log performance metrics for functions\"\"\"\n    \n    log_entry = {\n        'function': func_name,\n        'execution_time_seconds': execution_time,\n        'timestamp': datetime.now().isoformat()\n    }\n    \n    if data_size is not None:\n        log_entry['data_size'] = data_size\n        log_entry['performance_ratio'] = data_size / execution_time if execution_time > 0 else 0\n    \n    return log_entry\n\ndef create_backup_filename(base_name: str, extension: str = '.csv') -> str:\n    \"\"\"Create backup filename with timestamp\"\"\"\n    \n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    clean_base = clean_string(base_name)\n    \n    return f\"{clean_base}_backup_{timestamp}{extension}\"\n\nclass ConfigManager:\n    \"\"\"Simple configuration manager\"\"\"\n    \n    def __init__(self, config_dict: Dict[str, Any] = None):\n        self.config = config_dict or {}\n    \n    def get(self, key: str, default: Any = None) -> Any:\n        \"\"\"Get configuration value\"\"\"\n        keys = key.split('.')\n        value = self.config\n        \n        for k in keys:\n            if isinstance(value, dict) and k in value:\n                value = value[k]\n            else:\n                return default\n        \n        return value\n    \n    def set(self, key: str, value: Any):\n        \"\"\"Set configuration value\"\"\"\n        keys = key.split('.')\n        config = self.config\n        \n        for k in keys[:-1]:\n            if k not in config:\n                config[k] = {}\n            config = config[k]\n        \n        config[keys[-1]] = value\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Export configuration as dictionary\"\"\"\n        return self.config.copy()\n    \n    def from_dict(self, config_dict: Dict[str, Any]):\n        \"\"\"Import configuration from dictionary\"\"\"\n        self.config = config_dict.copy()\n","size_bytes":10150}},"version":1}